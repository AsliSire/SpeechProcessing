<html>
<head>
<title>tensorflow_backend.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #808080;}
.s3 { color: #6a8759;}
.s4 { color: #629755; font-style: italic;}
.s5 { color: #6897bb;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
tensorflow_backend.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">tensorflow </span><span class="s0">as </span><span class="s1">tf</span>
<span class="s0">from </span><span class="s1">tensorflow.python.training </span><span class="s0">import </span><span class="s1">moving_averages</span>
<span class="s0">from </span><span class="s1">tensorflow.python.ops </span><span class="s0">import </span><span class="s1">tensor_array_ops</span>
<span class="s0">from </span><span class="s1">tensorflow.python.ops </span><span class="s0">import </span><span class="s1">control_flow_ops</span>
<span class="s0">from </span><span class="s1">tensorflow.python.ops </span><span class="s0">import </span><span class="s1">functional_ops</span>
<span class="s0">from </span><span class="s1">tensorflow.python.ops </span><span class="s0">import </span><span class="s1">ctc_ops </span><span class="s0">as </span><span class="s1">ctc</span>
<span class="s0">from </span><span class="s1">tensorflow.python.ops </span><span class="s0">import </span><span class="s1">variables </span><span class="s0">as </span><span class="s1">tf_variables</span>

<span class="s0">from </span><span class="s1">collections </span><span class="s0">import </span><span class="s1">defaultdict</span>
<span class="s0">import </span><span class="s1">inspect</span>
<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>
<span class="s0">import </span><span class="s1">os</span>

<span class="s0">from </span><span class="s1">.common </span><span class="s0">import </span><span class="s1">floatx</span>
<span class="s0">from </span><span class="s1">.common </span><span class="s0">import </span><span class="s1">_EPSILON</span>
<span class="s0">from </span><span class="s1">.common </span><span class="s0">import </span><span class="s1">image_data_format</span>

<span class="s2"># Legacy functions</span>
<span class="s0">from </span><span class="s1">.common </span><span class="s0">import </span><span class="s1">set_image_dim_ordering</span>
<span class="s0">from </span><span class="s1">.common </span><span class="s0">import </span><span class="s1">image_dim_ordering</span>

py_all = all
py_sum = sum

<span class="s2"># INTERNAL UTILS</span>

# This is the default internal TF session used by Keras.
# It can be set manually via `set_session(sess)`.
<span class="s1">_SESSION = </span><span class="s0">None</span>

<span class="s2"># This dictionary holds a mapping {graph: learning_phase}.</span>
# A learning phase is a bool tensor used to run Keras models in
# either train mode (learning_phase == 1) or test mode (learning_phase == 0).
<span class="s1">_GRAPH_LEARNING_PHASES = {}</span>

<span class="s2"># This dictionary holds a mapping {graph: UID_DICT}.</span>
# each UID_DICT is a dictionary mapping name prefixes to a current index,
# used for generatic graph-specific string UIDs
# for various names (e.g. layer names).
<span class="s1">_GRAPH_UID_DICTS = {}</span>

<span class="s2"># This boolean flag can be set to True to leave variable initialization</span>
# up to the user.
# Change its value via `manual_variable_initialization(value)`.
<span class="s1">_MANUAL_VAR_INIT = </span><span class="s0">False</span>


def <span class="s1">get_uid(prefix=</span><span class="s3">''</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Get the uid for the default graph. 
 
    # Arguments 
        prefix: An optional prefix of the graph. 
 
    # Returns 
        A unique identifier for the graph. 
    &quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_GRAPH_UID_DICTS</span>
    graph = tf.get_default_graph()
    <span class="s0">if </span><span class="s1">graph </span><span class="s0">not in </span><span class="s1">_GRAPH_UID_DICTS:</span>
        _GRAPH_UID_DICTS[graph] = defaultdict(int)
    _GRAPH_UID_DICTS[graph][prefix] += <span class="s5">1</span>
    <span class="s0">return </span><span class="s1">_GRAPH_UID_DICTS[graph][prefix]</span>


<span class="s0">def </span><span class="s1">reset_uids():</span>
    <span class="s4">&quot;&quot;&quot;Reset graph identifiers.&quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_GRAPH_UID_DICTS</span>
    _GRAPH_UID_DICTS = {}


<span class="s0">def </span><span class="s1">clear_session():</span>
    <span class="s4">&quot;&quot;&quot;Destroys the current TF graph and creates a new one. 
 
    Useful to avoid clutter from old models / layers. 
    &quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_SESSION</span>
    <span class="s0">global </span><span class="s1">_GRAPH_LEARNING_PHASES</span>
    tf.reset_default_graph()
    reset_uids()
    _SESSION = <span class="s0">None</span>
    <span class="s1">phase = tf.placeholder(dtype=</span><span class="s3">'bool'</span><span class="s0">, </span><span class="s1">name=</span><span class="s3">'keras_learning_phase'</span><span class="s1">)</span>
    _GRAPH_LEARNING_PHASES = {}
    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase


<span class="s0">def </span><span class="s1">manual_variable_initialization(value):</span>
    <span class="s4">&quot;&quot;&quot;Sets the manual variable initialization flag. 
 
    This boolean flag determines whether 
    variables should be initialized 
    as they are instantiated (default), or if 
    the user should handle the initialization 
    (e.g. via `tf.initialize_all_variables()`). 
 
    # Arguments 
        value: Python boolean. 
    &quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_MANUAL_VAR_INIT</span>
    _MANUAL_VAR_INIT = value


<span class="s0">def </span><span class="s1">learning_phase():</span>
    <span class="s4">&quot;&quot;&quot;Returns the learning phase flag. 
 
    The learning phase flag is a bool tensor (0 = test, 1 = train) 
    to be passed as input to any Keras function 
    that uses a different behavior at train time and test time. 
 
    # Returns 
        Learning phase (scalar integer tensor or Python integer). 
    &quot;&quot;&quot;</span>
    <span class="s1">graph = tf.get_default_graph()</span>
    <span class="s0">if </span><span class="s1">graph </span><span class="s0">not in </span><span class="s1">_GRAPH_LEARNING_PHASES:</span>
        phase = tf.placeholder(dtype=<span class="s3">'bool'</span><span class="s0">,</span>
                               <span class="s1">name=</span><span class="s3">'keras_learning_phase'</span><span class="s1">)</span>
        _GRAPH_LEARNING_PHASES[graph] = phase
    <span class="s0">return </span><span class="s1">_GRAPH_LEARNING_PHASES[graph]</span>


<span class="s0">def </span><span class="s1">set_learning_phase(value):</span>
    <span class="s4">&quot;&quot;&quot;Sets the learning phase to a fixed value. 
 
    # Arguments 
        value: Learning phase value, either 0 or 1 (integers). 
 
    # Raises 
        ValueError: if `value` is neither `0` nor `1`. 
    &quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_GRAPH_LEARNING_PHASES</span>
    <span class="s0">if </span><span class="s1">value </span><span class="s0">not in </span><span class="s1">{</span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Expected learning phase to be '</span>
                         '0 or 1.'<span class="s1">)</span>
    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = value


<span class="s0">def </span><span class="s1">get_session():</span>
    <span class="s4">&quot;&quot;&quot;Returns the TF session to be used by the backend. 
 
    If a default TensorFlow session is available, we will return it. 
 
    Else, we will return the global Keras session. 
 
    If no global Keras session exists at this point: 
    we will create a new global session. 
 
    Note that you can manually set the global session 
    via `K.set_session(sess)`. 
 
    # Returns 
        A TensorFlow session. 
    &quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_SESSION</span>
    <span class="s0">if </span><span class="s1">tf.get_default_session() </span><span class="s0">is not None</span><span class="s1">:</span>
        session = tf.get_default_session()
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">_SESSION </span><span class="s0">is None</span><span class="s1">:</span>
            <span class="s0">if not </span><span class="s1">os.environ.get(</span><span class="s3">'OMP_NUM_THREADS'</span><span class="s1">):</span>
                config = tf.ConfigProto(allow_soft_placement=<span class="s0">True</span><span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                num_thread = int(os.environ.get(<span class="s3">'OMP_NUM_THREADS'</span><span class="s1">))</span>
                config = tf.ConfigProto(intra_op_parallelism_threads=num_thread<span class="s0">,</span>
                                        <span class="s1">allow_soft_placement=</span><span class="s0">True</span><span class="s1">)</span>
            _SESSION = tf.Session(config=config)
        session = _SESSION
    <span class="s0">if not </span><span class="s1">_MANUAL_VAR_INIT:</span>
        <span class="s0">with </span><span class="s1">session.graph.as_default():</span>
            _initialize_variables()
    <span class="s0">return </span><span class="s1">session</span>


<span class="s0">def </span><span class="s1">set_session(session):</span>
    <span class="s4">&quot;&quot;&quot;Sets the global TensorFlow session. 
 
    # Arguments 
        session: A TF Session. 
    &quot;&quot;&quot;</span>
    <span class="s0">global </span><span class="s1">_SESSION</span>
    _SESSION = session


<span class="s2"># VARIABLE MANIPULATION</span>

<span class="s0">def </span><span class="s1">_convert_string_dtype(dtype):</span>
    <span class="s4">&quot;&quot;&quot;Get the type from a string. 
 
    # Arguments 
        dtype: A string representation of a type. 
 
    # Returns 
        The type requested. 
 
    # Raises 
        ValueError: if `dtype` is not supported. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype == </span><span class="s3">'float16'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.float16</span>
    <span class="s0">if </span><span class="s1">dtype == </span><span class="s3">'float32'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.float32</span>
    <span class="s0">elif </span><span class="s1">dtype == </span><span class="s3">'float64'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.float64</span>
    <span class="s0">elif </span><span class="s1">dtype == </span><span class="s3">'int16'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.int16</span>
    <span class="s0">elif </span><span class="s1">dtype == </span><span class="s3">'int32'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.int32</span>
    <span class="s0">elif </span><span class="s1">dtype == </span><span class="s3">'int64'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.int64</span>
    <span class="s0">elif </span><span class="s1">dtype == </span><span class="s3">'uint8'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.int8</span>
    <span class="s0">elif </span><span class="s1">dtype == </span><span class="s3">'uint16'</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.uint16</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unsupported dtype:'</span><span class="s0">, </span><span class="s1">dtype)</span>


<span class="s0">def </span><span class="s1">_to_tensor(x</span><span class="s0">, </span><span class="s1">dtype):</span>
    <span class="s4">&quot;&quot;&quot;Convert the input `x` to a tensor of type `dtype`. 
 
    # Arguments 
        x: An object to be converted (numpy array, list, tensors). 
        dtype: The destination type. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">x = tf.convert_to_tensor(x)</span>
    <span class="s0">if </span><span class="s1">x.dtype != dtype:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s1">dtype)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">is_sparse(tensor):</span>
    <span class="s4">&quot;&quot;&quot;Returns whether a tensor is a sparse tensor. 
 
    # Arguments 
        tensor: A tensor instance. 
 
    # Returns 
        A boolean. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; a = K.placeholder((2, 2), sparse=False) 
        &gt;&gt;&gt; print(K.is_sparse(a)) 
        False 
        &gt;&gt;&gt; b = K.placeholder((2, 2), sparse=True) 
        &gt;&gt;&gt; print(K.is_sparse(b)) 
        True 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">isinstance(tensor</span><span class="s0">, </span><span class="s1">tf.SparseTensor)</span>


<span class="s0">def </span><span class="s1">to_dense(tensor):</span>
    <span class="s4">&quot;&quot;&quot;Converts a sparse tensor into a dense tensor and returns it. 
 
    # Arguments 
        tensor: A tensor instance (potentially sparse). 
 
    # Returns 
        A dense tensor. 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; b = K.placeholder((2, 2), sparse=True) 
        &gt;&gt;&gt; print(K.is_sparse(b)) 
        True 
        &gt;&gt;&gt; c = K.to_dense(b) 
        &gt;&gt;&gt; print(K.is_sparse(c)) 
        False 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">is_sparse(tensor):</span>
        <span class="s0">return </span><span class="s1">tf.sparse_tensor_to_dense(tensor)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tensor</span>


name_scope = tf.name_scope


<span class="s0">def </span><span class="s1">variable(value</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates a variable and returns it. 
 
    # Arguments 
        value: Numpy array, initial value of the tensor. 
        dtype: Tensor type. 
        name: Optional name string for the tensor. 
 
    # Returns 
        A variable instance (with Keras metadata included). 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; val = np.array([[1, 2], [3, 4]]) 
        &gt;&gt;&gt; kvar = K.variable(value=val, dtype='float64', name='example_var') 
        &gt;&gt;&gt; K.dtype(kvar) 
        'float64' 
        &gt;&gt;&gt; print(kvar) 
        example_var 
        &gt;&gt;&gt; kvar.eval() 
        array([[ 1.,  2.], 
               [ 3.,  4.]]) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">if </span><span class="s1">hasattr(value</span><span class="s0">, </span><span class="s3">'tocoo'</span><span class="s1">):</span>
        sparse_coo = value.tocoo()
        indices = np.concatenate((np.expand_dims(sparse_coo.row<span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,</span>
                                  <span class="s1">np.expand_dims(sparse_coo.col</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>
        v = tf.SparseTensor(indices=indices<span class="s0">,</span>
                            <span class="s1">values=sparse_coo.data</span><span class="s0">,</span>
                            <span class="s1">dense_shape=sparse_coo.shape)</span>
        v._keras_shape = sparse_coo.shape
        v._uses_learning_phase = <span class="s0">False</span>
        return <span class="s1">v</span>
    v = tf.Variable(value<span class="s0">, </span><span class="s1">dtype=_convert_string_dtype(dtype)</span><span class="s0">, </span><span class="s1">name=name)</span>
    <span class="s0">if </span><span class="s1">isinstance(value</span><span class="s0">, </span><span class="s1">np.ndarray):</span>
        v._keras_shape = value.shape
    <span class="s0">elif </span><span class="s1">hasattr(value</span><span class="s0">, </span><span class="s3">'get_shape'</span><span class="s1">):</span>
        v._keras_shape = tuple(map(int<span class="s0">, </span><span class="s1">value.get_shape()))</span>
    v._uses_learning_phase = <span class="s0">False</span>
    return <span class="s1">v</span>


<span class="s0">def </span><span class="s1">_initialize_variables():</span>
    <span class="s4">&quot;&quot;&quot;Utility to initialize uninitialized variables on the fly. 
    &quot;&quot;&quot;</span>
    <span class="s1">variables = tf.global_variables()</span>
    uninitialized_variables = []
    <span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">variables:</span>
        <span class="s0">if not </span><span class="s1">hasattr(v</span><span class="s0">, </span><span class="s3">'_keras_initialized'</span><span class="s1">) </span><span class="s0">or not </span><span class="s1">v._keras_initialized:</span>
            uninitialized_variables.append(v)
            v._keras_initialized = <span class="s0">True</span>
    if <span class="s1">uninitialized_variables:</span>
        sess = get_session()
        sess.run(tf.variables_initializer(uninitialized_variables))


<span class="s0">def </span><span class="s1">constant(value</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">shape=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Creates a constant tensor. 
 
    # Arguments 
        value: A constant value (or list) 
        dtype: The type of the elements of the resulting tensor. 
        shape: Optional dimensions of resulting tensor. 
        name: Optional name for the tensor. 
 
    # Returns 
        A Constant Tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">return </span><span class="s1">tf.constant(value</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">shape=shape</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">is_keras_tensor(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns whether `x` is a Keras tensor. 
 
    # Arguments 
        x: a potential tensor. 
 
    # Returns 
        A boolean: whether the argument is a Keras tensor. 
 
    # Raises 
        ValueError: in case `x` is not a symbolic tensor. 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; np_var = numpy.array([1, 2]) 
        &gt;&gt;&gt; K.is_keras_tensor(np_var) # A numpy array is not a symbolic yensor. 
        ValueError 
        &gt;&gt;&gt; k_var = tf.placeholder('float32', shape=(1,1)) 
        &gt;&gt;&gt; K.is_keras_tensor(k_var) # A variable created directly from tensorflow/theano is not a Keras tensor. 
        False 
        &gt;&gt;&gt; keras_var = K.variable(np_var) 
        &gt;&gt;&gt; K.is_keras_tensor(keras_var)  # A variable created with the keras backend is a Keras tensor. 
        True 
        &gt;&gt;&gt; keras_placeholder = K.placeholder(shape=(2, 4, 5)) 
        &gt;&gt;&gt; K.is_keras_tensor(keras_placeholder)  # A placeholder is a Keras tensor. 
        True 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if not </span><span class="s1">isinstance(x</span><span class="s0">, </span><span class="s1">(tf.Tensor</span><span class="s0">,</span>
                          <span class="s1">tf_variables.Variable</span><span class="s0">,</span>
                          <span class="s1">tf.SparseTensor)):</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unexpectedly found an instance of type `' </span><span class="s1">+ str(type(x)) + </span><span class="s3">'`. '</span>
                         'Expected a symbolic tensor instance.'<span class="s1">)</span>
    <span class="s0">return </span><span class="s1">hasattr(x</span><span class="s0">, </span><span class="s3">'_keras_history'</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">placeholder(shape=</span><span class="s0">None, </span><span class="s1">ndim=</span><span class="s0">None, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">sparse=</span><span class="s0">False, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates a placeholder tensor and returns it. 
 
    # Arguments 
        shape: Shape of the placeholder 
            (integer tuple, may include `None` entries). 
        ndim: Number of axes of the tensor. 
            At least one of {`shape`, `ndim`} must be specified. 
            If both are specified, `shape` is used. 
        dtype: Placeholder type. 
        sparse: Boolean, whether the placeholder should have a sparse type. 
        name: Optional name string for the placeholder. 
 
    # Returns 
        Tensor instance (with Keras metadata included). 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; input_ph = K.placeholder(shape=(2, 4, 5)) 
        &gt;&gt;&gt; input_ph._keras_shape 
        (2, 4, 5) 
        &gt;&gt;&gt; input_ph 
        &lt;tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32&gt; 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">if not </span><span class="s1">shape:</span>
        <span class="s0">if </span><span class="s1">ndim:</span>
            shape = tuple([<span class="s0">None for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(ndim)])</span>
    <span class="s0">if </span><span class="s1">sparse:</span>
        x = tf.sparse_placeholder(dtype<span class="s0">, </span><span class="s1">shape=shape</span><span class="s0">, </span><span class="s1">name=name)</span>
    <span class="s0">else</span><span class="s1">:</span>
        x = tf.placeholder(dtype<span class="s0">, </span><span class="s1">shape=shape</span><span class="s0">, </span><span class="s1">name=name)</span>
    x._keras_shape = shape
    x._uses_learning_phase = <span class="s0">False</span>
    return <span class="s1">x</span>


<span class="s0">def </span><span class="s1">shape(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the symbolic shape of a tensor or variable. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A symbolic shape (which is itself a tensor). 
 
    # Examples 
    ``` 
        # TensorFlow example 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; tf_session = K.get_session() 
        &gt;&gt;&gt; val = np.array([[1, 2], [3, 4]]) 
        &gt;&gt;&gt; kvar = K.variable(value=val) 
        &gt;&gt;&gt; input = keras.backend.placeholder(shape=(2, 4, 5)) 
        &gt;&gt;&gt; K.shape(kvar) 
        &lt;tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32&gt; 
        &gt;&gt;&gt; K.shape(input) 
        &lt;tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32&gt; 
        # To get integer shape (Instead, you can use K.int_shape(x)) 
        &gt;&gt;&gt; K.shape(kvar).eval(session=tf_session) 
        array([2, 2], dtype=int32) 
        &gt;&gt;&gt; K.shape(input).eval(session=tf_session) 
        array([2, 4, 5], dtype=int32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.shape(x)</span>


<span class="s0">def </span><span class="s1">int_shape(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the shape tensor or variable as a tuple of int or None entries. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tuple of integers (or None entries). 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; input = K.placeholder(shape=(2, 4, 5)) 
        &gt;&gt;&gt; K.int_shape(input) 
        (2, 4, 5) 
        &gt;&gt;&gt; val = np.array([[1, 2], [3, 4]]) 
        &gt;&gt;&gt; kvar = K.variable(value=val) 
        &gt;&gt;&gt; K.int_shape(kvar) 
        (2, 2) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">hasattr(x</span><span class="s0">, </span><span class="s3">'_keras_shape'</span><span class="s1">):</span>
        <span class="s0">return </span><span class="s1">x._keras_shape</span>
    shape = x.get_shape()
    <span class="s0">try</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tuple([i.__int__() </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">shape])</span>
    <span class="s0">except </span><span class="s1">ValueError:</span>
        <span class="s0">return None</span>


def <span class="s1">ndim(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the number of axes in a tensor, as an integer. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        Integer (scalar), number of axes. 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; input = K.placeholder(shape=(2, 4, 5)) 
        &gt;&gt;&gt; val = np.array([[1, 2], [3, 4]]) 
        &gt;&gt;&gt; kvar = K.variable(value=val) 
        &gt;&gt;&gt; K.ndim(input) 
        3 
        &gt;&gt;&gt; K.ndim(kvar) 
        2 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s1">dims = x.get_shape()._dims</span>
    <span class="s0">if </span><span class="s1">dims </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">len(dims)</span>
    <span class="s0">return None</span>


def <span class="s1">dtype(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the dtype of a Keras tensor or variable, as a string. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        String, dtype of `x`. 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5))) 
        'float32' 
        &gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5), dtype='float32')) 
        'float32' 
        &gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5), dtype='float64')) 
        'float64' 
        # Keras variable 
        &gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]])) 
        &gt;&gt;&gt; K.dtype(kvar) 
        'float32_ref' 
        &gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') 
        &gt;&gt;&gt; K.dtype(kvar) 
        'float32_ref' 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">x.dtype.name</span>


<span class="s0">def </span><span class="s1">eval(x):</span>
    <span class="s4">&quot;&quot;&quot;Evaluates the value of a variable. 
 
    # Arguments 
        x: A variable. 
 
    # Returns 
        A Numpy array. 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32') 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 1.,  2.], 
               [ 3.,  4.]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">to_dense(x).eval(session=get_session())</span>


<span class="s0">def </span><span class="s1">zeros(shape</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates an all-zeros variable and returns it. 
 
    # Arguments 
        shape: Tuple of integers, shape of returned Keras variable 
        dtype: String, data type of returned Keras variable 
        name: String, name of returned Keras variable 
 
    # Returns 
        A variable (including Keras metadata), filled with `0.0`. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; kvar = K.zeros((3,4)) 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 0.,  0.,  0.,  0.], 
               [ 0.,  0.,  0.,  0.], 
               [ 0.,  0.,  0.,  0.]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    shape = tuple(map(int<span class="s0">, </span><span class="s1">shape))</span>
    tf_dtype = _convert_string_dtype(dtype)
    <span class="s0">return </span><span class="s1">variable(tf.constant_initializer(</span><span class="s5">0.</span><span class="s0">, </span><span class="s1">dtype=tf_dtype)(shape)</span><span class="s0">,</span>
                    <span class="s1">dtype</span><span class="s0">, </span><span class="s1">name)</span>


<span class="s0">def </span><span class="s1">ones(shape</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates an all-ones tensor variable and returns it. 
 
    # Arguments 
        shape: Tuple of integers, shape of returned Keras variable. 
        dtype: String, data type of returned Keras variable. 
        name: String, name of returned Keras variable. 
 
    # Returns 
        A Keras variable, filled with `1.0`. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; kvar = K.ones((3,4)) 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 1.,  1.,  1.,  1.], 
               [ 1.,  1.,  1.,  1.], 
               [ 1.,  1.,  1.,  1.]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    shape = tuple(map(int<span class="s0">, </span><span class="s1">shape))</span>
    tf_dtype = _convert_string_dtype(dtype)
    <span class="s0">return </span><span class="s1">variable(tf.constant_initializer(</span><span class="s5">1.</span><span class="s0">, </span><span class="s1">dtype=tf_dtype)(shape)</span><span class="s0">,</span>
                    <span class="s1">dtype</span><span class="s0">, </span><span class="s1">name)</span>


<span class="s0">def </span><span class="s1">eye(size</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiate an identity matrix and returns it. 
 
    # Arguments 
        size: Integer, number of rows/columns. 
        dtype: String, data type of returned Keras variable. 
        name: String, name of returned Keras variable. 
 
    # Returns 
        A Keras variable, an identity matrix. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; kvar = K.eye(3) 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 1.,  0.,  0.], 
               [ 0.,  1.,  0.], 
               [ 0.,  0.,  1.]], dtype=float32) 
    ``` 
 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">variable(np.eye(size)</span><span class="s0">, </span><span class="s1">dtype</span><span class="s0">, </span><span class="s1">name)</span>


<span class="s0">def </span><span class="s1">zeros_like(x</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates an all-zeros variable of the same shape as another tensor. 
 
    # Arguments 
        x: Keras variable or Keras tensor. 
        dtype: String, dtype of returned Keras variable. 
             None uses the dtype of x. 
        name: String, name for the variable to create. 
 
    # Returns 
        A Keras variable with the shape of x filled with zeros. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; kvar = K.variable(np.random.random((2,3))) 
        &gt;&gt;&gt; kvar_zeros = K.zeros_like(kvar) 
        &gt;&gt;&gt; K.eval(kvar_zeros) 
        array([[ 0.,  0.,  0.], 
               [ 0.,  0.,  0.]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.zeros_like(x</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">ones_like(x</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates an all-ones variable of the same shape as another tensor. 
 
    # Arguments 
        x: Keras variable or tensor. 
        dtype: String, dtype of returned Keras variable. 
             None uses the dtype of x. 
        name: String, name for the variable to create. 
 
    # Returns 
        A Keras variable with the shape of x filled with ones. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; kvar = K.variable(np.random.random((2,3))) 
        &gt;&gt;&gt; kvar_ones = K.ones_like(kvar) 
        &gt;&gt;&gt; K.eval(kvar_ones) 
        array([[ 1.,  1.,  1.], 
               [ 1.,  1.,  1.]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.ones_like(x</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">identity(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns a tensor with the same content as the input tensor. 
 
    # Arguments 
        x: The input tensor. 
 
    # Returns 
        A tensor of the same shape, type and content. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.identity(x)</span>


<span class="s0">def </span><span class="s1">random_uniform_variable(shape</span><span class="s0">, </span><span class="s1">low</span><span class="s0">, </span><span class="s1">high</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None,</span>
                            <span class="s1">name=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates a variable with values drawn from a uniform distribution. 
 
    # Arguments 
        shape: Tuple of integers, shape of returned Keras variable. 
        low: Float, lower boundary of the output interval. 
        high: Float, upper boundary of the output interval. 
        dtype: String, dtype of returned Keras variable. 
        name: String, name of returned Keras variable. 
        seed: Integer, random seed. 
 
    # Returns 
        A Keras variable, filled with drawn samples. 
 
    # Example 
    ```python 
        # TensorFlow example 
        &gt;&gt;&gt; kvar = K.random_uniform_variable((2,3), 0, 1) 
        &gt;&gt;&gt; kvar 
        &lt;tensorflow.python.ops.variables.Variable object at 0x10ab40b10&gt; 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 0.10940075,  0.10047495,  0.476143  ], 
               [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    shape = tuple(map(int<span class="s0">, </span><span class="s1">shape))</span>
    tf_dtype = _convert_string_dtype(dtype)
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s2"># ensure that randomness is conditioned by the Numpy RNG</span>
        <span class="s1">seed = np.random.randint(</span><span class="s5">10e8</span><span class="s1">)</span>
    value = tf.random_uniform_initializer(
        low<span class="s0">, </span><span class="s1">high</span><span class="s0">, </span><span class="s1">dtype=tf_dtype</span><span class="s0">, </span><span class="s1">seed=seed)(shape)</span>
    <span class="s0">return </span><span class="s1">variable(value</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">random_normal_variable(shape</span><span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">scale</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None,</span>
                           <span class="s1">name=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates a variable with values drawn from a normal distribution. 
 
    # Arguments 
        shape: Tuple of integers, shape of returned Keras variable. 
        mean: Float, mean of the normal distribution. 
        scale: Float, standard deviation of the normal distribution. 
        dtype: String, dtype of returned Keras variable. 
        name: String, name of returned Keras variable. 
        seed: Integer, random seed. 
 
    # Returns 
        A Keras variable, filled with drawn samples. 
 
    # Example 
    ```python 
        # TensorFlow example 
        &gt;&gt;&gt; kvar = K.random_normal_variable((2,3), 0, 1) 
        &gt;&gt;&gt; kvar 
        &lt;tensorflow.python.ops.variables.Variable object at 0x10ab12dd0&gt; 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 1.19591331,  0.68685907, -0.63814116], 
               [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    shape = tuple(map(int<span class="s0">, </span><span class="s1">shape))</span>
    tf_dtype = _convert_string_dtype(dtype)
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s2"># ensure that randomness is conditioned by the Numpy RNG</span>
        <span class="s1">seed = np.random.randint(</span><span class="s5">10e8</span><span class="s1">)</span>
    value = tf.random_normal_initializer(
        mean<span class="s0">, </span><span class="s1">scale</span><span class="s0">, </span><span class="s1">dtype=tf_dtype</span><span class="s0">, </span><span class="s1">seed=seed)(shape)</span>
    <span class="s0">return </span><span class="s1">variable(value</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">count_params(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the number of scalars in a Keras variable. 
 
    # Arguments 
        x: Keras variable. 
 
    # Returns 
        Integer, the number of scalars in `x`. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; kvar = K.zeros((2,3)) 
        &gt;&gt;&gt; K.count_params(kvar) 
        6 
        &gt;&gt;&gt; K.eval(kvar) 
        array([[ 0.,  0.,  0.], 
               [ 0.,  0.,  0.]], dtype=float32) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s1">shape = x.get_shape()</span>
    <span class="s0">return </span><span class="s1">np.prod([shape[i]._value </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(shape))])</span>


<span class="s0">def </span><span class="s1">cast(x</span><span class="s0">, </span><span class="s1">dtype):</span>
    <span class="s4">&quot;&quot;&quot;Casts a tensor to a different dtype and returns it. 
 
    You can cast a Keras variable but it still returns a Keras tensor. 
 
    # Arguments 
        x: Keras tensor (or variable). 
        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`). 
 
    # Returns 
        Keras tensor with dtype `dtype`. 
 
    # Example 
    ```python 
        &gt;&gt;&gt; from keras import backend as K 
        &gt;&gt;&gt; input = K.placeholder((2, 3), dtype='float32') 
        &gt;&gt;&gt; input 
        &lt;tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32&gt; 
        # It doesn't work in-place as below. 
        &gt;&gt;&gt; K.cast(input, dtype='float16') 
        &lt;tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16&gt; 
        &gt;&gt;&gt; input 
        &lt;tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32&gt; 
        # you need to assign it. 
        &gt;&gt;&gt; input = K.cast(input, dtype='float16') 
        &gt;&gt;&gt; input 
        &lt;tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16&gt; 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.cast(x</span><span class="s0">, </span><span class="s1">dtype)</span>


<span class="s2"># UPDATES OPS</span>


<span class="s0">def </span><span class="s1">update(x</span><span class="s0">, </span><span class="s1">new_x):</span>
    <span class="s4">&quot;&quot;&quot;Update the value of `x` to `new_x`. 
 
    # Arguments 
        x: A Variable. 
        new_x: A tensor of same shape as `x`. 
 
    # Returns 
        The variable `x` updated. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.assign(x</span><span class="s0">, </span><span class="s1">new_x)</span>


<span class="s0">def </span><span class="s1">update_add(x</span><span class="s0">, </span><span class="s1">increment):</span>
    <span class="s4">&quot;&quot;&quot;Update the value of `x` by adding `increment`. 
 
    # Arguments 
        x: A Variable. 
        increment: A tensor of same shape as `x`. 
 
    # Returns 
        The variable `x` updated. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.assign_add(x</span><span class="s0">, </span><span class="s1">increment)</span>


<span class="s0">def </span><span class="s1">update_sub(x</span><span class="s0">, </span><span class="s1">decrement):</span>
    <span class="s4">&quot;&quot;&quot;Update the value of `x` by subtracting `decrement`. 
 
    # Arguments 
        x: A Variable. 
        decrement: A tensor of same shape as `x`. 
 
    # Returns 
        The variable `x` updated. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.assign_sub(x</span><span class="s0">, </span><span class="s1">decrement)</span>


<span class="s0">def </span><span class="s1">moving_average_update(x</span><span class="s0">, </span><span class="s1">value</span><span class="s0">, </span><span class="s1">momentum):</span>
    <span class="s4">&quot;&quot;&quot;Compute the moving average of a variable. 
 
    # Arguments 
        x: A Variable. 
        value: A tensor with the same shape as `variable`. 
        momentum: The moving average momentum. 
 
    # Returns 
        An Operation to update the variable.&quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">moving_averages.assign_moving_average(</span>
        x<span class="s0">, </span><span class="s1">value</span><span class="s0">, </span><span class="s1">momentum</span><span class="s0">, </span><span class="s1">zero_debias=</span><span class="s0">False</span><span class="s1">)</span>


<span class="s2"># LINEAR ALGEBRA</span>

<span class="s0">def </span><span class="s1">dot(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Multiplies 2 tensors (and/or variables) and returns a *tensor*. 
 
    When attempting to multiply a nD tensor 
    with a nD tensor, it reproduces the Theano behavior. 
    (e.g. `(2, 3) * (4, 3, 5) -&gt; (2, 4, 5)`) 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A tensor, dot product of `x` and `y`. 
 
    # Examples 
    ```python 
        # dot product between tensors 
        &gt;&gt;&gt; x = K.placeholder(shape=(2, 3)) 
        &gt;&gt;&gt; y = K.placeholder(shape=(3, 4)) 
        &gt;&gt;&gt; xy = K.dot(x, y) 
        &gt;&gt;&gt; xy 
        &lt;tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32&gt; 
    ``` 
 
    ```python 
        # dot product between tensors 
        &gt;&gt;&gt; x = K.placeholder(shape=(32, 28, 3)) 
        &gt;&gt;&gt; y = K.placeholder(shape=(3, 4)) 
        &gt;&gt;&gt; xy = K.dot(x, y) 
        &gt;&gt;&gt; xy 
        &lt;tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32&gt; 
    ``` 
 
    ```python 
        # Theano-like behavior example 
        &gt;&gt;&gt; x = K.random_uniform_variable(shape=(2, 3), low=0, high=1) 
        &gt;&gt;&gt; y = K.ones((4, 3, 5)) 
        &gt;&gt;&gt; xy = K.dot(x, y) 
        &gt;&gt;&gt; K.int_shape(xy) 
        (2, 4, 5) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">ndim(x) </span><span class="s0">is not None and </span><span class="s1">(ndim(x) &gt; </span><span class="s5">2 </span><span class="s0">or </span><span class="s1">ndim(y) &gt; </span><span class="s5">2</span><span class="s1">):</span>
        x_shape = []
        <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">s </span><span class="s0">in </span><span class="s1">zip(int_shape(x)</span><span class="s0">, </span><span class="s1">tf.unstack(tf.shape(x))):</span>
            <span class="s0">if </span><span class="s1">i </span><span class="s0">is not None</span><span class="s1">:</span>
                x_shape.append(i)
            <span class="s0">else</span><span class="s1">:</span>
                x_shape.append(s)
        x_shape = tuple(x_shape)
        y_shape = []
        <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">s </span><span class="s0">in </span><span class="s1">zip(int_shape(y)</span><span class="s0">, </span><span class="s1">tf.unstack(tf.shape(y))):</span>
            <span class="s0">if </span><span class="s1">i </span><span class="s0">is not None</span><span class="s1">:</span>
                y_shape.append(i)
            <span class="s0">else</span><span class="s1">:</span>
                y_shape.append(s)
        y_shape = tuple(y_shape)
        y_permute_dim = list(range(ndim(y)))
        y_permute_dim = [y_permute_dim.pop(-<span class="s5">2</span><span class="s1">)] + y_permute_dim</span>
        xt = tf.reshape(x<span class="s0">, </span><span class="s1">[-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">x_shape[-</span><span class="s5">1</span><span class="s1">]])</span>
        yt = tf.reshape(tf.transpose(y<span class="s0">, </span><span class="s1">perm=y_permute_dim)</span><span class="s0">, </span><span class="s1">[y_shape[-</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">])</span>
        <span class="s0">return </span><span class="s1">tf.reshape(tf.matmul(xt</span><span class="s0">, </span><span class="s1">yt)</span><span class="s0">,</span>
                          <span class="s1">x_shape[:-</span><span class="s5">1</span><span class="s1">] + y_shape[:-</span><span class="s5">2</span><span class="s1">] + y_shape[-</span><span class="s5">1</span><span class="s1">:])</span>
    <span class="s0">if </span><span class="s1">is_sparse(x):</span>
        out = tf.sparse_tensor_dense_matmul(x<span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">else</span><span class="s1">:</span>
        out = tf.matmul(x<span class="s0">, </span><span class="s1">y)</span>
    <span class="s0">return </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">batch_dot(x</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">axes=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Batchwise dot product. 
 
    `batch_dot` is used to compute dot product of `x` and `y` when 
    `x` and `y` are data in batch, i.e. in a shape of 
    `(batch_size, :)`. 
    `batch_dot` results in a tensor or variable with less dimensions 
    than the input. If the number of dimensions is reduced to 1, 
    we use `expand_dims` to make sure that ndim is at least 2. 
 
    # Arguments 
        x: Keras tensor or variable with `ndim &gt;= 2`. 
        y: Keras tensor or variable with `ndim &gt;= 2`. 
        axes: list of (or single) int with target dimensions. 
            The lengths of `axes[0]` and `axes[1]` should be the same. 
 
    # Returns 
        A tensor with shape equal to the concatenation of `x`'s shape 
        (less the dimension that was summed over) and `y`'s shape 
        (less the batch dimension and the dimension that was summed over). 
        If the final rank is 1, we reshape it to `(batch_size, 1)`. 
 
    # Examples 
        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]` 
        `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal 
        of `x.dot(y.T)`, although we never have to calculate the off-diagonal 
        elements. 
 
        Shape inference: 
        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`. 
        If `axes` is (1, 2), to find the output shape of resultant tensor, 
            loop through each dimension in `x`'s shape and `y`'s shape: 
 
        * `x.shape[0]` : 100 : append to output shape 
        * `x.shape[1]` : 20 : do not append to output shape, 
            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1) 
        * `y.shape[0]` : 100 : do not append to output shape, 
            always ignore first dimension of `y` 
        * `y.shape[1]` : 30 : append to output shape 
        * `y.shape[2]` : 20 : do not append to output shape, 
            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2) 
        `output_shape` = `(100, 30)` 
 
    ```python 
        &gt;&gt;&gt; x_batch = K.ones(shape=(32, 20, 1)) 
        &gt;&gt;&gt; y_batch = K.ones(shape=(32, 30, 20)) 
        &gt;&gt;&gt; xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2]) 
        &gt;&gt;&gt; K.int_shape(xy_batch_dot) 
        (32, 1, 30) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance(axes</span><span class="s0">, </span><span class="s1">int):</span>
        axes = (axes<span class="s0">, </span><span class="s1">axes)</span>
    x_ndim = ndim(x)
    y_ndim = ndim(y)
    <span class="s0">if </span><span class="s1">x_ndim &gt; y_ndim:</span>
        diff = x_ndim - y_ndim
        y = tf.reshape(y<span class="s0">, </span><span class="s1">tf.concat([tf.shape(y)</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">] * (diff)]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s0">elif </span><span class="s1">y_ndim &gt; x_ndim:</span>
        diff = y_ndim - x_ndim
        x = tf.reshape(x<span class="s0">, </span><span class="s1">tf.concat([tf.shape(x)</span><span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s1">] * (diff)]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">))</span>
    <span class="s0">else</span><span class="s1">:</span>
        diff = <span class="s5">0</span>
    <span class="s0">if </span><span class="s1">ndim(x) == </span><span class="s5">2 </span><span class="s0">and </span><span class="s1">ndim(y) == </span><span class="s5">2</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">axes[</span><span class="s5">0</span><span class="s1">] == axes[</span><span class="s5">1</span><span class="s1">]:</span>
            out = tf.reduce_sum(tf.multiply(x<span class="s0">, </span><span class="s1">y)</span><span class="s0">, </span><span class="s1">axes[</span><span class="s5">0</span><span class="s1">])</span>
        <span class="s0">else</span><span class="s1">:</span>
            out = tf.reduce_sum(tf.multiply(tf.transpose(x<span class="s0">, </span><span class="s1">[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">y)</span><span class="s0">, </span><span class="s1">axes[</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">axes </span><span class="s0">is not None</span><span class="s1">:</span>
            adj_x = <span class="s0">None if </span><span class="s1">axes[</span><span class="s5">0</span><span class="s1">] == ndim(x) - </span><span class="s5">1 </span><span class="s0">else True</span>
            <span class="s1">adj_y = </span><span class="s0">True if </span><span class="s1">axes[</span><span class="s5">1</span><span class="s1">] == ndim(y) - </span><span class="s5">1 </span><span class="s0">else None</span>
        else<span class="s1">:</span>
            adj_x = <span class="s0">None</span>
            <span class="s1">adj_y = </span><span class="s0">None</span>
        <span class="s1">out = tf.matmul(x</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">adjoint_a=adj_x</span><span class="s0">, </span><span class="s1">adjoint_b=adj_y)</span>
    <span class="s0">if </span><span class="s1">diff:</span>
        <span class="s0">if </span><span class="s1">x_ndim &gt; y_ndim:</span>
            idx = x_ndim + y_ndim - <span class="s5">3</span>
        <span class="s0">else</span><span class="s1">:</span>
            idx = x_ndim - <span class="s5">1</span>
        <span class="s1">out = tf.squeeze(out</span><span class="s0">, </span><span class="s1">list(range(idx</span><span class="s0">, </span><span class="s1">idx + diff)))</span>
    <span class="s0">if </span><span class="s1">ndim(out) == </span><span class="s5">1</span><span class="s1">:</span>
        out = expand_dims(out<span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">out</span>


<span class="s0">def </span><span class="s1">transpose(x):</span>
    <span class="s4">&quot;&quot;&quot;Transposes a tensor and returns it. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
 
    # Examples 
    ```python 
        &gt;&gt;&gt; var = K.variable([[1, 2, 3], [4, 5, 6]]) 
        &gt;&gt;&gt; K.eval(var) 
        array([[ 1.,  2.,  3.], 
               [ 4.,  5.,  6.]], dtype=float32) 
        &gt;&gt;&gt; var_transposed = K.transpose(var) 
        &gt;&gt;&gt; K.eval(var_transposed) 
        array([[ 1.,  4.], 
               [ 2.,  5.], 
               [ 3.,  6.]], dtype=float32) 
    ``` 
 
    ```python 
        &gt;&gt;&gt; input = K.placeholder((2, 3)) 
        &gt;&gt;&gt; input 
        &lt;tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32&gt; 
        &gt;&gt;&gt; input_transposed = K.transpose(input) 
        &gt;&gt;&gt; input_transposed 
        &lt;tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32&gt; 
 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.transpose(x)</span>


<span class="s0">def </span><span class="s1">gather(reference</span><span class="s0">, </span><span class="s1">indices):</span>
    <span class="s4">&quot;&quot;&quot;Retrieves the elements of indices `indices` in the tensor `reference`. 
 
    # Arguments 
        reference: A tensor. 
        indices: An integer tensor of indices. 
 
    # Returns 
        A tensor of same type as `reference`. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.gather(reference</span><span class="s0">, </span><span class="s1">indices)</span>


<span class="s2"># ELEMENT-WISE OPERATIONS</span>

<span class="s0">def </span><span class="s1">_normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim):</span>
    <span class="s4">&quot;&quot;&quot;Converts negative axes to positive values. 
 
    # Arguments 
        axis: Integer axis (possibly negative). 
        ndim: Rank of the tensor considered. 
 
    # Returns 
        Positive integer axis. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance(axis</span><span class="s0">, </span><span class="s1">tuple):</span>
        axis = list(axis)
    <span class="s0">if </span><span class="s1">isinstance(axis</span><span class="s0">, </span><span class="s1">list):</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">a </span><span class="s0">in </span><span class="s1">enumerate(axis):</span>
            <span class="s0">if </span><span class="s1">a </span><span class="s0">is not None and </span><span class="s1">a &lt; </span><span class="s5">0</span><span class="s1">:</span>
                axis[i] = a % ndim
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">axis </span><span class="s0">is not None and </span><span class="s1">axis &lt; </span><span class="s5">0</span><span class="s1">:</span>
            axis %= ndim
    <span class="s0">return </span><span class="s1">axis</span>


<span class="s0">def </span><span class="s1">max(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Maximum value in a tensor. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to find maximum values. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, 
            the reduced dimension is retained with length 1. 
 
    # Returns 
        A tensor with maximum values of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.reduce_max(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">min(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Minimum value in a tensor. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to find minimum values. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, 
            the reduced dimension is retained with length 1. 
 
    # Returns 
        A tensor with miminum values of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.reduce_min(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">sum(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Sum of the values in a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to sum over. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, 
            the reduced dimension is retained with length 1. 
 
    # Returns 
        A tensor with sum of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.reduce_sum(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">prod(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Multiplies the values in a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to compute the product. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, 
            the reduced dimension is retained with length 1. 
 
    # Returns 
        A tensor with the product of elements of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.reduce_prod(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">cumsum(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Cumulative sum of the values in a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to compute the sum. 
 
    # Returns 
        A tensor of the cumulative sum of values of `x` along `axis`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.cumsum(x</span><span class="s0">, </span><span class="s1">axis=axis)</span>


<span class="s0">def </span><span class="s1">cumprod(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Cumulative product of the values in a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to compute the product. 
 
    # Returns 
        A tensor of the cumulative product of values of `x` along `axis`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.cumprod(x</span><span class="s0">, </span><span class="s1">axis=axis)</span>


<span class="s0">def </span><span class="s1">var(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Variance of a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to compute the variance. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, 
            the reduced dimension is retained with length 1. 
 
    # Returns 
        A tensor with the variance of elements of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">if </span><span class="s1">x.dtype.base_dtype == tf.bool:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s1">floatx())</span>
    m = tf.reduce_mean(x<span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=</span><span class="s0">True</span><span class="s1">)</span>
    devs_squared = tf.square(x - m)
    <span class="s0">return </span><span class="s1">tf.reduce_mean(devs_squared</span><span class="s0">,</span>
                          <span class="s1">reduction_indices=axis</span><span class="s0">,</span>
                          <span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">std(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Standard deviation of a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to compute the standard deviation. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, 
            the reduced dimension is retained with length 1. 
 
    # Returns 
        A tensor with the standard deviation of elements of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.sqrt(var(x</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keepdims=keepdims))</span>


<span class="s0">def </span><span class="s1">mean(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Mean of a tensor, alongside the specified axis. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: A list of integer. Axes to compute the mean. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1 for each entry in `axis`. If `keep_dims` is `True`, 
            the reduced dimensions are retained with length 1. 
 
    # Returns 
        A tensor with the mean of elements of `x`. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">if </span><span class="s1">x.dtype.base_dtype == tf.bool:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s1">floatx())</span>
    <span class="s0">return </span><span class="s1">tf.reduce_mean(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">any(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Bitwise reduction (logical OR). 
 
    # Arguments 
        x: Tensor or variable. 
        axis: axis along which to perform the reduction. 
        keepdims: whether the drop or broadcast the reduction axes. 
 
    # Returns 
        A uint8 tensor (0s and 1s). 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    x = tf.cast(x<span class="s0">, </span><span class="s1">tf.bool)</span>
    <span class="s0">return </span><span class="s1">tf.reduce_any(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">all(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Bitwise reduction (logical AND). 
 
    # Arguments 
        x: Tensor or variable. 
        axis: axis along which to perform the reduction. 
        keepdims: whether the drop or broadcast the reduction axes. 
 
    # Returns 
        A uint8 tensor (0s and 1s). 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    x = tf.cast(x<span class="s0">, </span><span class="s1">tf.bool)</span>
    <span class="s0">return </span><span class="s1">tf.reduce_all(x</span><span class="s0">, </span><span class="s1">reduction_indices=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">argmax(x</span><span class="s0">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns the index of the maximum value along an axis. 
 
    # Arguments 
        x: Tensor or variable. 
        axis: axis along which to perform the reduction. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.argmax(x</span><span class="s0">, </span><span class="s1">axis)</span>


<span class="s0">def </span><span class="s1">argmin(x</span><span class="s0">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns the index of the minimum value along an axis. 
 
    # Arguments 
        x: Tensor or variable. 
        axis: axis along which to perform the reduction. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.argmin(x</span><span class="s0">, </span><span class="s1">axis)</span>


<span class="s0">def </span><span class="s1">square(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise square. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.square(x)</span>


<span class="s0">def </span><span class="s1">abs(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise absolute value. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.abs(x)</span>


<span class="s0">def </span><span class="s1">sqrt(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise square root. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">zero = _to_tensor(</span><span class="s5">0.</span><span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
    inf = _to_tensor(np.inf<span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
    x = tf.clip_by_value(x<span class="s0">, </span><span class="s1">zero</span><span class="s0">, </span><span class="s1">inf)</span>
    <span class="s0">return </span><span class="s1">tf.sqrt(x)</span>


<span class="s0">def </span><span class="s1">exp(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise exponential. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.exp(x)</span>


<span class="s0">def </span><span class="s1">log(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise log. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.log(x)</span>


<span class="s0">def </span><span class="s1">logsumexp(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s0">None, </span><span class="s1">keepdims=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Computes log(sum(exp(elements across dimensions of a tensor))). 
 
    This function is more numerically stable than log(sum(exp(x))). 
    It avoids overflows caused by taking the exp of large inputs and 
    underflows caused by taking the log of small inputs. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: An integer, the axis to reduce over. 
        keepdims: A boolean, whether to keep the dimensions or not. 
            If `keepdims` is `False`, the rank of the tensor is reduced 
            by 1. If `keepdims` is `True`, the reduced dimension is 
            retained with length 1. 
 
    # Returns 
        The reduced tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">axis = _normalize_axis(axis</span><span class="s0">, </span><span class="s1">ndim(x))</span>
    <span class="s0">return </span><span class="s1">tf.reduce_logsumexp(x</span><span class="s0">, </span><span class="s1">axis=axis</span><span class="s0">, </span><span class="s1">keep_dims=keepdims)</span>


<span class="s0">def </span><span class="s1">round(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise rounding to the closest integer. 
 
    In case of tie, the rounding mode used is &quot;half to even&quot;. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.round(x)</span>


<span class="s0">def </span><span class="s1">sign(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise sign. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.sign(x)</span>


<span class="s0">def </span><span class="s1">pow(x</span><span class="s0">, </span><span class="s1">a):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise exponentiation. 
 
    # Arguments 
        x: Tensor or variable. 
        a: Python integer. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.pow(x</span><span class="s0">, </span><span class="s1">a)</span>


<span class="s0">def </span><span class="s1">clip(x</span><span class="s0">, </span><span class="s1">min_value</span><span class="s0">, </span><span class="s1">max_value):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise value clipping. 
 
    # Arguments 
        x: Tensor or variable. 
        min_value: Python float or integer. 
        max_value: Python float or integer. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">max_value </span><span class="s0">is not None and </span><span class="s1">max_value &lt; min_value:</span>
        max_value = min_value
    <span class="s0">if </span><span class="s1">max_value </span><span class="s0">is None</span><span class="s1">:</span>
        max_value = np.inf
    min_value = _to_tensor(min_value<span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
    max_value = _to_tensor(max_value<span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
    <span class="s0">return </span><span class="s1">tf.clip_by_value(x</span><span class="s0">, </span><span class="s1">min_value</span><span class="s0">, </span><span class="s1">max_value)</span>


<span class="s0">def </span><span class="s1">equal(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise equality between two tensors. 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A bool tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.equal(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">not_equal(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise inequality between two tensors. 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A bool tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.not_equal(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">greater(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise truth value of (x &gt; y). 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A bool tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.greater(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">greater_equal(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise truth value of (x &gt;= y). 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A bool tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.greater_equal(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">less(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise truth value of (x &lt; y). 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A bool tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.less(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">less_equal(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise truth value of (x &lt;= y). 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A bool tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.less_equal(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">maximum(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise maximum of two tensors. 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.maximum(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">minimum(x</span><span class="s0">, </span><span class="s1">y):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise minimum of two tensors. 
 
    # Arguments 
        x: Tensor or variable. 
        y: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.minimum(x</span><span class="s0">, </span><span class="s1">y)</span>


<span class="s0">def </span><span class="s1">sin(x):</span>
    <span class="s4">&quot;&quot;&quot;Computes sin of x element-wise. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.sin(x)</span>


<span class="s0">def </span><span class="s1">cos(x):</span>
    <span class="s4">&quot;&quot;&quot;Computes cos of x element-wise. 
 
    # Arguments 
        x: Tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.cos(x)</span>


<span class="s0">def </span><span class="s1">normalize_batch_in_training(x</span><span class="s0">, </span><span class="s1">gamma</span><span class="s0">, </span><span class="s1">beta</span><span class="s0">,</span>
                                <span class="s1">reduction_axes</span><span class="s0">, </span><span class="s1">epsilon=</span><span class="s5">1e-3</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Computes mean and std for batch then apply batch_normalization on batch. 
 
    # Arguments 
        x: Input tensor or variable. 
        gamma: Tensor by which to scale the input. 
        beta: Tensor with which to center the input. 
        reduction_axes: iterable of integers, 
            axes over which to normalize. 
        epsilon: Fuzz factor. 
 
    # Returns 
        A tuple length of 3, `(normalized_tensor, mean, variance)`. 
    &quot;&quot;&quot;</span>
    <span class="s1">mean</span><span class="s0">, </span><span class="s1">var = tf.nn.moments(x</span><span class="s0">, </span><span class="s1">reduction_axes</span><span class="s0">,</span>
                              <span class="s1">shift=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None, </span><span class="s1">keep_dims=</span><span class="s0">False</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">sorted(reduction_axes) == list(range(ndim(x)))[:-</span><span class="s5">1</span><span class="s1">]:</span>
        normed = tf.nn.batch_normalization(x<span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">var</span><span class="s0">,</span>
                                           <span class="s1">beta</span><span class="s0">, </span><span class="s1">gamma</span><span class="s0">,</span>
                                           <span class="s1">epsilon)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s2"># need broadcasting</span>
        <span class="s1">target_shape = []</span>
        <span class="s0">for </span><span class="s1">axis </span><span class="s0">in </span><span class="s1">range(ndim(x)):</span>
            <span class="s0">if </span><span class="s1">axis </span><span class="s0">in </span><span class="s1">reduction_axes:</span>
                target_shape.append(<span class="s5">1</span><span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                target_shape.append(tf.shape(x)[axis])
        target_shape = tf.stack(target_shape)

        broadcast_mean = tf.reshape(mean<span class="s0">, </span><span class="s1">target_shape)</span>
        broadcast_var = tf.reshape(var<span class="s0">, </span><span class="s1">target_shape)</span>
        <span class="s0">if </span><span class="s1">gamma </span><span class="s0">is None</span><span class="s1">:</span>
            broadcast_gamma = <span class="s0">None</span>
        else<span class="s1">:</span>
            broadcast_gamma = tf.reshape(gamma<span class="s0">, </span><span class="s1">target_shape)</span>
        <span class="s0">if </span><span class="s1">beta </span><span class="s0">is None</span><span class="s1">:</span>
            broadcast_beta = <span class="s0">None</span>
        else<span class="s1">:</span>
            broadcast_beta = tf.reshape(beta<span class="s0">, </span><span class="s1">target_shape)</span>
        normed = tf.nn.batch_normalization(x<span class="s0">, </span><span class="s1">broadcast_mean</span><span class="s0">, </span><span class="s1">broadcast_var</span><span class="s0">,</span>
                                           <span class="s1">broadcast_beta</span><span class="s0">, </span><span class="s1">broadcast_gamma</span><span class="s0">,</span>
                                           <span class="s1">epsilon)</span>
    <span class="s0">return </span><span class="s1">normed</span><span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">var</span>


<span class="s0">def </span><span class="s1">batch_normalization(x</span><span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">var</span><span class="s0">, </span><span class="s1">beta</span><span class="s0">, </span><span class="s1">gamma</span><span class="s0">, </span><span class="s1">epsilon=</span><span class="s5">1e-3</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Applies batch normalization on x given mean, var, beta and gamma. 
 
    I.e. returns: 
    `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta` 
 
    # Arguments 
        x: Input tensor or variable. 
        mean: Mean of batch. 
        var: Variance of batch. 
        beta: Tensor with which to center the input. 
        gamma: Tensor by which to scale the input. 
        epsilon: Fuzz factor. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.batch_normalization(x</span><span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">var</span><span class="s0">, </span><span class="s1">beta</span><span class="s0">, </span><span class="s1">gamma</span><span class="s0">, </span><span class="s1">epsilon)</span>


<span class="s2"># SHAPE OPERATIONS</span>

<span class="s0">def </span><span class="s1">concatenate(tensors</span><span class="s0">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Concatenates a list of tensors alongside the specified axis. 
 
    # Arguments 
        tensors: list of tensors to concatenate. 
        axis: concatenation axis. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">axis &lt; </span><span class="s5">0</span><span class="s1">:</span>
        rank = ndim(tensors[<span class="s5">0</span><span class="s1">])</span>
        <span class="s0">if </span><span class="s1">rank:</span>
            axis %= rank
        <span class="s0">else</span><span class="s1">:</span>
            axis = <span class="s5">0</span>

    <span class="s0">if </span><span class="s1">py_all([is_sparse(x) </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">tensors]):</span>
        <span class="s0">return </span><span class="s1">tf.sparse_concat(axis</span><span class="s0">, </span><span class="s1">tensors)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.concat([to_dense(x) </span><span class="s0">for </span><span class="s1">x </span><span class="s0">in </span><span class="s1">tensors]</span><span class="s0">, </span><span class="s1">axis)</span>


<span class="s0">def </span><span class="s1">reshape(x</span><span class="s0">, </span><span class="s1">shape):</span>
    <span class="s4">&quot;&quot;&quot;Reshapes a tensor to the specified shape. 
 
    # Arguments 
        x: Tensor or variable. 
        shape: Target shape tuple. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.reshape(x</span><span class="s0">, </span><span class="s1">shape)</span>


<span class="s0">def </span><span class="s1">permute_dimensions(x</span><span class="s0">, </span><span class="s1">pattern):</span>
    <span class="s4">&quot;&quot;&quot;Permutes axes in a tensor. 
 
    # Arguments 
        x: Tensor or variable. 
        pattern: A tuple of 
            dimension indices, e.g. `(0, 2, 1)`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.transpose(x</span><span class="s0">, </span><span class="s1">perm=pattern)</span>


<span class="s0">def </span><span class="s1">resize_images(x</span><span class="s0">, </span><span class="s1">height_factor</span><span class="s0">, </span><span class="s1">width_factor</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Resizes the images contained in a 4D tensor. 
 
    # Arguments 
        x: Tensor or variable to resize. 
        height_factor: Positive integer. 
        width_factor: Positive integer. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
 
    # Raises 
        ValueError: if `data_format` is neither `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        original_shape = int_shape(x)
        new_shape = tf.shape(x)[<span class="s5">2</span><span class="s1">:]</span>
        new_shape *= tf.constant(np.array([height_factor<span class="s0">, </span><span class="s1">width_factor]).astype(</span><span class="s3">'int32'</span><span class="s1">))</span>
        x = permute_dimensions(x<span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">1</span><span class="s1">])</span>
        x = tf.image.resize_nearest_neighbor(x<span class="s0">, </span><span class="s1">new_shape)</span>
        x = permute_dimensions(x<span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">])</span>
        x.set_shape((<span class="s0">None, None, </span><span class="s1">original_shape[</span><span class="s5">2</span><span class="s1">] * height_factor </span><span class="s0">if </span><span class="s1">original_shape[</span><span class="s5">2</span><span class="s1">] </span><span class="s0">is not None else None,</span>
                     <span class="s1">original_shape[</span><span class="s5">3</span><span class="s1">] * width_factor </span><span class="s0">if </span><span class="s1">original_shape[</span><span class="s5">3</span><span class="s1">] </span><span class="s0">is not None else None</span><span class="s1">))</span>
        <span class="s0">return </span><span class="s1">x</span>
    <span class="s0">elif </span><span class="s1">data_format == </span><span class="s3">'channels_last'</span><span class="s1">:</span>
        original_shape = int_shape(x)
        new_shape = tf.shape(x)[<span class="s5">1</span><span class="s1">:</span><span class="s5">3</span><span class="s1">]</span>
        new_shape *= tf.constant(np.array([height_factor<span class="s0">, </span><span class="s1">width_factor]).astype(</span><span class="s3">'int32'</span><span class="s1">))</span>
        x = tf.image.resize_nearest_neighbor(x<span class="s0">, </span><span class="s1">new_shape)</span>
        x.set_shape((<span class="s0">None, </span><span class="s1">original_shape[</span><span class="s5">1</span><span class="s1">] * height_factor </span><span class="s0">if </span><span class="s1">original_shape[</span><span class="s5">1</span><span class="s1">] </span><span class="s0">is not None else None,</span>
                     <span class="s1">original_shape[</span><span class="s5">2</span><span class="s1">] * width_factor </span><span class="s0">if </span><span class="s1">original_shape[</span><span class="s5">2</span><span class="s1">] </span><span class="s0">is not None else None, None</span><span class="s1">))</span>
        <span class="s0">return </span><span class="s1">x</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Invalid data_format:'</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">resize_volumes(x</span><span class="s0">, </span><span class="s1">depth_factor</span><span class="s0">, </span><span class="s1">height_factor</span><span class="s0">, </span><span class="s1">width_factor</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Resizes the volume contained in a 5D tensor. 
 
    # Arguments 
        x: Tensor or variable to resize. 
        depth_factor: Positive integer. 
        height_factor: Positive integer. 
        width_factor: Positive integer. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
 
    # Raises 
        ValueError: if `data_format` is neither `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        output = repeat_elements(x<span class="s0">, </span><span class="s1">depth_factor</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">2</span><span class="s1">)</span>
        output = repeat_elements(output<span class="s0">, </span><span class="s1">height_factor</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">3</span><span class="s1">)</span>
        output = repeat_elements(output<span class="s0">, </span><span class="s1">width_factor</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">4</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">output</span>
    <span class="s0">elif </span><span class="s1">data_format == </span><span class="s3">'channels_last'</span><span class="s1">:</span>
        output = repeat_elements(x<span class="s0">, </span><span class="s1">depth_factor</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">1</span><span class="s1">)</span>
        output = repeat_elements(output<span class="s0">, </span><span class="s1">height_factor</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">2</span><span class="s1">)</span>
        output = repeat_elements(output<span class="s0">, </span><span class="s1">width_factor</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">3</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">output</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Invalid data_format:'</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">repeat_elements(x</span><span class="s0">, </span><span class="s1">rep</span><span class="s0">, </span><span class="s1">axis):</span>
    <span class="s4">&quot;&quot;&quot;Repeats the elements of a tensor along an axis, like `np.repeat`. 
 
    If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output 
    will have shape `(s1, s2 * rep, s3)`. 
 
    # Arguments 
        x: Tensor or variable. 
        rep: Python integer, number of times to repeat. 
        axis: Axis along which to repeat. 
 
    # Raises 
        ValueError: In case `x.shape[axis]` is undefined. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">x_shape = x.get_shape().as_list()</span>
    <span class="s0">if </span><span class="s1">x_shape[axis] </span><span class="s0">is None</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Axis ' </span><span class="s1">+ str(axis) + </span><span class="s3">' of input tensor '</span>
                         'should have a defined dimension, but is None. '
                         'Full tensor shape: ' <span class="s1">+ str(tuple(x_shape)) + </span><span class="s3">'. '</span>
                         'Typically you need to pass a fully-defined '
                         '`input_shape` argument to your first layer.'<span class="s1">)</span>
    <span class="s2"># slices along the repeat axis</span>
    <span class="s1">splits = tf.split(value=x</span><span class="s0">, </span><span class="s1">num_or_size_splits=x_shape[axis]</span><span class="s0">, </span><span class="s1">axis=axis)</span>
    <span class="s2"># repeat each slice the given number of reps</span>
    <span class="s1">x_rep = [s </span><span class="s0">for </span><span class="s1">s </span><span class="s0">in </span><span class="s1">splits </span><span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range(rep)]</span>
    <span class="s0">return </span><span class="s1">concatenate(x_rep</span><span class="s0">, </span><span class="s1">axis)</span>


<span class="s0">def </span><span class="s1">repeat(x</span><span class="s0">, </span><span class="s1">n):</span>
    <span class="s4">&quot;&quot;&quot;Repeats a 2D tensor. 
 
    if `x` has shape (samples, dim) and `n` is `2`, 
    the output will have shape `(samples, 2, dim)`. 
 
    # Arguments 
        x: Tensor or variable. 
        n: Python integer, number of times to repeat. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">assert </span><span class="s1">ndim(x) == </span><span class="s5">2</span>
    <span class="s1">x = tf.expand_dims(x</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>
    pattern = tf.stack([<span class="s5">1</span><span class="s0">, </span><span class="s1">n</span><span class="s0">, </span><span class="s5">1</span><span class="s1">])</span>
    <span class="s0">return </span><span class="s1">tf.tile(x</span><span class="s0">, </span><span class="s1">pattern)</span>


<span class="s0">def </span><span class="s1">arange(start</span><span class="s0">, </span><span class="s1">stop=</span><span class="s0">None, </span><span class="s1">step=</span><span class="s5">1</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s3">'int32'</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Creates a 1D tensor containing a sequence of integers. 
 
    The function arguments use the same convention as 
    Theano's arange: if only one argument is provided, 
    it is in fact the &quot;stop&quot; argument. 
 
    The default type of the returned tensor is `'int32'` to 
    match TensorFlow's default. 
 
    # Arguments 
        start: Start value. 
        stop: Stop value. 
        step: Difference between two successive values. 
        dtype: Integer dtype to use. 
 
    # Returns 
        An integer tensor. 
 
    &quot;&quot;&quot;</span>
    <span class="s2"># Match the behavior of numpy and Theano by returning an empty seqence.</span>
    <span class="s0">if </span><span class="s1">stop </span><span class="s0">is None and </span><span class="s1">start &lt; </span><span class="s5">0</span><span class="s1">:</span>
        start = <span class="s5">0</span>
    <span class="s1">result = tf.range(start</span><span class="s0">, </span><span class="s1">limit=stop</span><span class="s0">, </span><span class="s1">delta=step</span><span class="s0">, </span><span class="s1">name=</span><span class="s3">'arange'</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">dtype != </span><span class="s3">'int32'</span><span class="s1">:</span>
        result = cast(result<span class="s0">, </span><span class="s1">dtype)</span>
    <span class="s0">return </span><span class="s1">result</span>


<span class="s0">def </span><span class="s1">tile(x</span><span class="s0">, </span><span class="s1">n):</span>
    <span class="s4">&quot;&quot;&quot;Creates a tensor by tiling `x` by `n`. 
 
    # Arguments 
        x: A tensor or variable 
        n: A list of integer. The length must be the same as the number of 
            dimensions in `x`. 
 
    # Returns 
        A tiled tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance(n</span><span class="s0">, </span><span class="s1">int):</span>
        n = [n]
    <span class="s0">return </span><span class="s1">tf.tile(x</span><span class="s0">, </span><span class="s1">n)</span>


<span class="s0">def </span><span class="s1">flatten(x):</span>
    <span class="s4">&quot;&quot;&quot;Flatten a tensor. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor, reshaped into 1-D 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.reshape(x</span><span class="s0">, </span><span class="s1">[-</span><span class="s5">1</span><span class="s1">])</span>


<span class="s0">def </span><span class="s1">batch_flatten(x):</span>
    <span class="s4">&quot;&quot;&quot;Turn a nD tensor into a 2D tensor with same 0th dimension. 
 
    In other words, it flattens each data samples of a batch. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">x = tf.reshape(x</span><span class="s0">, </span><span class="s1">tf.stack([-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">prod(shape(x)[</span><span class="s5">1</span><span class="s1">:])]))</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">expand_dims(x</span><span class="s0">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Adds a 1-sized dimension at index &quot;axis&quot;. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: Position where to add a new axis. 
 
    # Returns 
        A tensor with expanded dimensions. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.expand_dims(x</span><span class="s0">, </span><span class="s1">axis)</span>


<span class="s0">def </span><span class="s1">squeeze(x</span><span class="s0">, </span><span class="s1">axis):</span>
    <span class="s4">&quot;&quot;&quot;Removes a 1-dimension from the tensor at index &quot;axis&quot;. 
 
    # Arguments 
        x: A tensor or variable. 
        axis: Axis to drop. 
 
    # Returns 
        A tensor with the same data as `x` but reduced dimensions. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.squeeze(x</span><span class="s0">, </span><span class="s1">[axis])</span>


<span class="s0">def </span><span class="s1">temporal_padding(x</span><span class="s0">, </span><span class="s1">padding=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)):</span>
    <span class="s4">&quot;&quot;&quot;Pads the middle dimension of a 3D tensor. 
 
    # Arguments 
        x: Tensor or variable. 
        padding: Tuple of 2 integers, how many zeros to 
            add at the start and end of dim 1. 
 
    # Returns 
        A padded 3D tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">assert </span><span class="s1">len(padding) == </span><span class="s5">2</span>
    <span class="s1">pattern = [[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">[padding[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]]</span>
    <span class="s0">return </span><span class="s1">tf.pad(x</span><span class="s0">, </span><span class="s1">pattern)</span>


<span class="s0">def </span><span class="s1">spatial_2d_padding(x</span><span class="s0">, </span><span class="s1">padding=((</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Pads the 2nd and 3rd dimensions of a 4D tensor. 
 
    # Arguments 
        x: Tensor or variable. 
        padding: Tuple of 2 tuples, padding pattern. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A padded 4D tensor. 
 
    # Raises 
        ValueError: if `data_format` is neither `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
    &quot;&quot;&quot;</span>
    <span class="s0">assert </span><span class="s1">len(padding) == </span><span class="s5">2</span>
    <span class="s0">assert </span><span class="s1">len(padding[</span><span class="s5">0</span><span class="s1">]) == </span><span class="s5">2</span>
    <span class="s0">assert </span><span class="s1">len(padding[</span><span class="s5">1</span><span class="s1">]) == </span><span class="s5">2</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        pattern = [[<span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">list(padding[</span><span class="s5">0</span><span class="s1">])</span><span class="s0">,</span>
                   <span class="s1">list(padding[</span><span class="s5">1</span><span class="s1">])]</span>
    <span class="s0">else</span><span class="s1">:</span>
        pattern = [[<span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">,</span>
                   <span class="s1">list(padding[</span><span class="s5">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">list(padding[</span><span class="s5">1</span><span class="s1">])</span><span class="s0">,</span>
                   <span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]]</span>
    <span class="s0">return </span><span class="s1">tf.pad(x</span><span class="s0">, </span><span class="s1">pattern)</span>


<span class="s0">def </span><span class="s1">spatial_3d_padding(x</span><span class="s0">, </span><span class="s1">padding=((</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Pads 5D tensor with zeros along the depth, height, width dimensions. 
 
    Pads these dimensions with respectively 
    &quot;padding[0]&quot;, &quot;padding[1]&quot; and &quot;padding[2]&quot; zeros left and right. 
 
    For 'channels_last' data_format, 
    the 2nd, 3rd and 4th dimension will be padded. 
    For 'channels_first' data_format, 
    the 3rd, 4th and 5th dimension will be padded. 
 
    # Arguments 
        x: Tensor or variable. 
        padding: Tuple of 3 tuples, padding pattern. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A padded 5D tensor. 
 
    # Raises 
        ValueError: if `data_format` is neither `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    &quot;&quot;&quot;</span>
    <span class="s0">assert </span><span class="s1">len(padding) == </span><span class="s5">3</span>
    <span class="s0">assert </span><span class="s1">len(padding[</span><span class="s5">0</span><span class="s1">]) == </span><span class="s5">2</span>
    <span class="s0">assert </span><span class="s1">len(padding[</span><span class="s5">1</span><span class="s1">]) == </span><span class="s5">2</span>
    <span class="s0">assert </span><span class="s1">len(padding[</span><span class="s5">2</span><span class="s1">]) == </span><span class="s5">2</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        pattern = [
            [<span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[padding[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[padding[</span><span class="s5">1</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">1</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[padding[</span><span class="s5">2</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">2</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]]</span>
        ]
    <span class="s0">else</span><span class="s1">:</span>
        pattern = [
            [<span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">[padding[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">0</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[padding[</span><span class="s5">1</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">1</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[padding[</span><span class="s5">2</span><span class="s1">][</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">padding[</span><span class="s5">2</span><span class="s1">][</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">,</span>
            <span class="s1">[</span><span class="s5">0</span><span class="s0">, </span><span class="s5">0</span><span class="s1">]</span>
        ]
    <span class="s0">return </span><span class="s1">tf.pad(x</span><span class="s0">, </span><span class="s1">pattern)</span>


<span class="s0">def </span><span class="s1">stack(x</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Stacks a list of rank `R` tensors into a rank `R+1` tensor. 
 
    # Arguments 
        x: List of tensors. 
        axis: Axis along which to perform stacking. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.stack(x</span><span class="s0">, </span><span class="s1">axis=axis)</span>


<span class="s0">def </span><span class="s1">one_hot(indices</span><span class="s0">, </span><span class="s1">num_classes):</span>
    <span class="s4">&quot;&quot;&quot;Computes the one-hot representation of an integer tensor. 
 
    # Arguments 
        indices: nD integer tensor of shape 
            `(batch_size, dim1, dim2, ... dim(n-1))` 
        num_classes: Integer, number of classes to consider. 
 
    # Returns 
        (n + 1)D one hot representation of the input 
        with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)` 
 
    # Returns 
        The one-hot tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.one_hot(indices</span><span class="s0">, </span><span class="s1">depth=num_classes</span><span class="s0">, </span><span class="s1">axis=-</span><span class="s5">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">reverse(x</span><span class="s0">, </span><span class="s1">axes):</span>
    <span class="s4">&quot;&quot;&quot;Reverse a tensor along the specified axes. 
 
    # Arguments 
        x: Tensor to reverse. 
        axes: Integer or iterable of integers. 
            Axes to reverse. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">isinstance(axes</span><span class="s0">, </span><span class="s1">int):</span>
        axes = [axes]
    <span class="s0">return </span><span class="s1">tf.reverse(x</span><span class="s0">, </span><span class="s1">axes)</span>


<span class="s2"># VALUE MANIPULATION</span>


<span class="s0">def </span><span class="s1">get_value(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the value of a variable. 
 
    # Arguments 
        x: input variable. 
 
    # Returns 
        A Numpy array. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">x.eval(session=get_session())</span>


<span class="s0">def </span><span class="s1">batch_get_value(ops):</span>
    <span class="s4">&quot;&quot;&quot;Returns the value of more than one tensor variable. 
 
    # Arguments 
        ops: list of ops to run. 
 
    # Returns 
        A list of Numpy arrays. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">ops:</span>
        <span class="s0">return </span><span class="s1">get_session().run(ops)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">[]</span>


<span class="s0">def </span><span class="s1">set_value(x</span><span class="s0">, </span><span class="s1">value):</span>
    <span class="s4">&quot;&quot;&quot;Sets the value of a variable, from a Numpy array. 
 
    # Arguments 
        x: Tensor to set to a new value. 
        value: Value to set the tensor to, as a Numpy array 
            (of the same shape). 
    &quot;&quot;&quot;</span>
    <span class="s1">value = np.asarray(value)</span>
    tf_dtype = _convert_string_dtype(x.dtype.name.split(<span class="s3">'_'</span><span class="s1">)[</span><span class="s5">0</span><span class="s1">])</span>
    <span class="s0">if </span><span class="s1">hasattr(x</span><span class="s0">, </span><span class="s3">'_assign_placeholder'</span><span class="s1">):</span>
        assign_placeholder = x._assign_placeholder
        assign_op = x._assign_op
    <span class="s0">else</span><span class="s1">:</span>
        assign_placeholder = tf.placeholder(tf_dtype<span class="s0">, </span><span class="s1">shape=value.shape)</span>
        assign_op = x.assign(assign_placeholder)
        x._assign_placeholder = assign_placeholder
        x._assign_op = assign_op
    get_session().run(assign_op<span class="s0">, </span><span class="s1">feed_dict={assign_placeholder: value})</span>


<span class="s0">def </span><span class="s1">batch_set_value(tuples):</span>
    <span class="s4">&quot;&quot;&quot;Sets the values of many tensor variables at once. 
 
    # Arguments 
        tuples: a list of tuples `(tensor, value)`. 
            `value` should be a Numpy array. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">tuples:</span>
        assign_ops = []
        feed_dict = {}
        <span class="s0">for </span><span class="s1">x</span><span class="s0">, </span><span class="s1">value </span><span class="s0">in </span><span class="s1">tuples:</span>
            value = np.asarray(value)
            tf_dtype = _convert_string_dtype(x.dtype.name.split(<span class="s3">'_'</span><span class="s1">)[</span><span class="s5">0</span><span class="s1">])</span>
            <span class="s0">if </span><span class="s1">hasattr(x</span><span class="s0">, </span><span class="s3">'_assign_placeholder'</span><span class="s1">):</span>
                assign_placeholder = x._assign_placeholder
                assign_op = x._assign_op
            <span class="s0">else</span><span class="s1">:</span>
                assign_placeholder = tf.placeholder(tf_dtype<span class="s0">,</span>
                                                    <span class="s1">shape=value.shape)</span>
                assign_op = x.assign(assign_placeholder)
                x._assign_placeholder = assign_placeholder
                x._assign_op = assign_op
            assign_ops.append(assign_op)
            feed_dict[assign_placeholder] = value
        get_session().run(assign_ops<span class="s0">, </span><span class="s1">feed_dict=feed_dict)</span>


<span class="s0">def </span><span class="s1">get_variable_shape(x):</span>
    <span class="s4">&quot;&quot;&quot;Returns the shape of a variable. 
 
    # Arguments 
        x: A variable. 
 
    # Returns 
        A tuple of integers. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">int_shape(x)</span>


<span class="s0">def </span><span class="s1">print_tensor(x</span><span class="s0">, </span><span class="s1">message=</span><span class="s3">''</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Prints `message` and the tensor value when evaluated. 
 
    # Arguments 
        x: Tensor to print. 
        message: Message to print jointly with the tensor. 
 
    # Returns 
        The same tensor `x`, unchanged. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.Print(x</span><span class="s0">, </span><span class="s1">[x]</span><span class="s0">, </span><span class="s1">message)</span>


<span class="s2"># GRAPH MANIPULATION</span>

<span class="s0">class </span><span class="s1">Function(object):</span>
    <span class="s4">&quot;&quot;&quot;Runs a computation graph. 
 
    # Arguments 
        inputs: Feed placeholders to the computation graph. 
        outputs: Output tensors to fetch. 
        updates: Additional update ops to be run at function call. 
        name: a name to help users identify what this function does. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">, </span><span class="s1">inputs</span><span class="s0">, </span><span class="s1">outputs</span><span class="s0">, </span><span class="s1">updates=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None, </span><span class="s1">**session_kwargs):</span>
        updates = updates <span class="s0">or </span><span class="s1">[]</span>
        <span class="s0">if not </span><span class="s1">isinstance(inputs</span><span class="s0">, </span><span class="s1">(list</span><span class="s0">, </span><span class="s1">tuple)):</span>
            <span class="s0">raise </span><span class="s1">TypeError(</span><span class="s3">'`inputs` to a TensorFlow backend function '</span>
                            'should be a list or tuple.'<span class="s1">)</span>
        <span class="s0">if not </span><span class="s1">isinstance(outputs</span><span class="s0">, </span><span class="s1">(list</span><span class="s0">, </span><span class="s1">tuple)):</span>
            <span class="s0">raise </span><span class="s1">TypeError(</span><span class="s3">'`outputs` of a TensorFlow backend function '</span>
                            'should be a list or tuple.'<span class="s1">)</span>
        <span class="s0">if not </span><span class="s1">isinstance(updates</span><span class="s0">, </span><span class="s1">(list</span><span class="s0">, </span><span class="s1">tuple)):</span>
            <span class="s0">raise </span><span class="s1">TypeError(</span><span class="s3">'`updates` in a TensorFlow backend function '</span>
                            'should be a list or tuple.'<span class="s1">)</span>
        self.inputs = list(inputs)
        self.outputs = list(outputs)
        <span class="s0">with </span><span class="s1">tf.control_dependencies(self.outputs):</span>
            updates_ops = []
            <span class="s0">for </span><span class="s1">update </span><span class="s0">in </span><span class="s1">updates:</span>
                <span class="s0">if </span><span class="s1">isinstance(update</span><span class="s0">, </span><span class="s1">tuple):</span>
                    p<span class="s0">, </span><span class="s1">new_p = update</span>
                    updates_ops.append(tf.assign(p<span class="s0">, </span><span class="s1">new_p))</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s2"># assumed already an op</span>
                    <span class="s1">updates_ops.append(update)</span>
            self.updates_op = tf.group(*updates_ops)
        self.name = name
        self.session_kwargs = session_kwargs

    <span class="s0">def </span><span class="s1">__call__(self</span><span class="s0">, </span><span class="s1">inputs):</span>
        <span class="s0">if not </span><span class="s1">isinstance(inputs</span><span class="s0">, </span><span class="s1">(list</span><span class="s0">, </span><span class="s1">tuple)):</span>
            <span class="s0">raise </span><span class="s1">TypeError(</span><span class="s3">'`inputs` should be a list or tuple.'</span><span class="s1">)</span>
        feed_dict = {}
        <span class="s0">for </span><span class="s1">tensor</span><span class="s0">, </span><span class="s1">value </span><span class="s0">in </span><span class="s1">zip(self.inputs</span><span class="s0">, </span><span class="s1">inputs):</span>
            <span class="s0">if </span><span class="s1">is_sparse(tensor):</span>
                sparse_coo = value.tocoo()
                indices = np.concatenate((np.expand_dims(sparse_coo.row<span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,</span>
                                          <span class="s1">np.expand_dims(sparse_coo.col</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>
                value = (indices<span class="s0">, </span><span class="s1">sparse_coo.data</span><span class="s0">, </span><span class="s1">sparse_coo.shape)</span>
            feed_dict[tensor] = value
        session = get_session()
        updated = session.run(self.outputs + [self.updates_op]<span class="s0">,</span>
                              <span class="s1">feed_dict=feed_dict</span><span class="s0">,</span>
                              <span class="s1">**self.session_kwargs)</span>
        <span class="s0">return </span><span class="s1">updated[:len(self.outputs)]</span>


<span class="s0">def </span><span class="s1">function(inputs</span><span class="s0">, </span><span class="s1">outputs</span><span class="s0">, </span><span class="s1">updates=</span><span class="s0">None, </span><span class="s1">**kwargs):</span>
    <span class="s4">&quot;&quot;&quot;Instantiates a Keras function. 
 
    # Arguments 
        inputs: List of placeholder tensors. 
        outputs: List of output tensors. 
        updates: List of update ops. 
        **kwargs: Passed to `tf.Session.run`. 
 
    # Returns 
        Output values as Numpy arrays. 
 
    # Raises 
        ValueError: if invalid kwargs are passed in. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">kwargs:</span>
        <span class="s0">for </span><span class="s1">key </span><span class="s0">in </span><span class="s1">kwargs:</span>
            <span class="s0">if </span><span class="s1">(key </span><span class="s0">not in </span><span class="s1">inspect.getargspec(tf.Session.run)[</span><span class="s5">0</span><span class="s1">] </span><span class="s0">and</span>
                    <span class="s1">key </span><span class="s0">not in </span><span class="s1">inspect.getargspec(Function.__init__)[</span><span class="s5">0</span><span class="s1">]):</span>
                msg = <span class="s3">'Invalid argument &quot;%s&quot; passed to K.function with Tensorflow backend' </span><span class="s1">% key</span>
                <span class="s0">raise </span><span class="s1">ValueError(msg)</span>
    <span class="s0">return </span><span class="s1">Function(inputs</span><span class="s0">, </span><span class="s1">outputs</span><span class="s0">, </span><span class="s1">updates=updates</span><span class="s0">, </span><span class="s1">**kwargs)</span>


<span class="s0">def </span><span class="s1">gradients(loss</span><span class="s0">, </span><span class="s1">variables):</span>
    <span class="s4">&quot;&quot;&quot;Returns the gradients of `variables` w.r.t. `loss`. 
 
    # Arguments 
        loss: Scalar tensor to minimize. 
        variables: List of variables. 
 
    # Returns 
        A gradients tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.gradients(loss</span><span class="s0">, </span><span class="s1">variables</span><span class="s0">, </span><span class="s1">colocate_gradients_with_ops=</span><span class="s0">True</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">stop_gradient(variables):</span>
    <span class="s4">&quot;&quot;&quot;Returns `variables` but with zero gradient w.r.t. every other variable. 
 
    # Arguments 
        variables: List of variables. 
 
    # Returns 
        The same list of variables. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.stop_gradient(variables)</span>


<span class="s2"># CONTROL FLOW</span>

<span class="s0">def </span><span class="s1">rnn(step_function</span><span class="s0">, </span><span class="s1">inputs</span><span class="s0">, </span><span class="s1">initial_states</span><span class="s0">,</span>
        <span class="s1">go_backwards=</span><span class="s0">False, </span><span class="s1">mask=</span><span class="s0">None, </span><span class="s1">constants=</span><span class="s0">None,</span>
        <span class="s1">unroll=</span><span class="s0">False, </span><span class="s1">input_length=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Iterates over the time dimension of a tensor. 
 
    # Arguments 
        step_function: RNN step function. 
            Parameters: 
                input: tensor with shape `(samples, ...)` (no time dimension), 
                    representing input for the batch of samples at a certain 
                    time step. 
                states: list of tensors. 
            Returns: 
                output: tensor with shape `(samples, output_dim)` 
                    (no time dimension). 
                new_states: list of tensors, same length and shapes 
                    as 'states'. The first state in the list must be the 
                    output tensor at the previous timestep. 
        inputs: tensor of temporal data of shape `(samples, time, ...)` 
            (at least 3D). 
        initial_states: tensor with shape (samples, output_dim) 
            (no time dimension), 
            containing the initial values for the states used in 
            the step function. 
        go_backwards: boolean. If True, do the iteration over the time 
            dimension in reverse order and return the reversed sequence. 
        mask: binary tensor with shape `(samples, time, 1)`, 
            with a zero for every element that is masked. 
        constants: a list of constant values passed at each step. 
        unroll: whether to unroll the RNN or to use a symbolic loop (`while_loop` or `scan` depending on backend). 
        input_length: not relevant in the TensorFlow implementation. 
            Must be specified if using unrolling with Theano. 
 
    # Returns 
        A tuple, `(last_output, outputs, new_states)`. 
 
            last_output: the latest output of the rnn, of shape `(samples, ...)` 
            outputs: tensor with shape `(samples, time, ...)` where each 
                entry `outputs[s, t]` is the output of the step function 
                at time `t` for sample `s`. 
            new_states: list of tensors, latest states returned by 
                the step function, of shape `(samples, ...)`. 
 
    # Raises 
        ValueError: if input dimension is less than 3. 
        ValueError: if `unroll` is `True` but input timestep is not a fixed number. 
        ValueError: if `mask` is provided (not `None`) but states is not provided 
            (`len(states)` == 0). 
    &quot;&quot;&quot;</span>
    <span class="s1">ndim = len(inputs.get_shape())</span>
    <span class="s0">if </span><span class="s1">ndim &lt; </span><span class="s5">3</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Input should be at least 3D.'</span><span class="s1">)</span>
    axes = [<span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">] + list(range(</span><span class="s5">2</span><span class="s0">, </span><span class="s1">ndim))</span>
    inputs = tf.transpose(inputs<span class="s0">, </span><span class="s1">(axes))</span>

    <span class="s0">if </span><span class="s1">mask </span><span class="s0">is not None</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">mask.dtype != tf.bool:</span>
            mask = tf.cast(mask<span class="s0">, </span><span class="s1">tf.bool)</span>
        <span class="s0">if </span><span class="s1">len(mask.get_shape()) == ndim - </span><span class="s5">1</span><span class="s1">:</span>
            mask = expand_dims(mask)
        mask = tf.transpose(mask<span class="s0">, </span><span class="s1">axes)</span>

    <span class="s0">if </span><span class="s1">constants </span><span class="s0">is None</span><span class="s1">:</span>
        constants = []

    <span class="s0">if </span><span class="s1">unroll:</span>
        <span class="s0">if not </span><span class="s1">inputs.get_shape()[</span><span class="s5">0</span><span class="s1">]:</span>
            <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unrolling requires a '</span>
                             'fixed number of timesteps.'<span class="s1">)</span>
        states = initial_states
        successive_states = []
        successive_outputs = []

        input_list = tf.unstack(inputs)
        <span class="s0">if </span><span class="s1">go_backwards:</span>
            input_list.reverse()

        <span class="s0">if </span><span class="s1">mask </span><span class="s0">is not None</span><span class="s1">:</span>
            mask_list = tf.unstack(mask)
            <span class="s0">if </span><span class="s1">go_backwards:</span>
                mask_list.reverse()

            <span class="s0">for </span><span class="s1">inp</span><span class="s0">, </span><span class="s1">mask_t </span><span class="s0">in </span><span class="s1">zip(input_list</span><span class="s0">, </span><span class="s1">mask_list):</span>
                output<span class="s0">, </span><span class="s1">new_states = step_function(inp</span><span class="s0">, </span><span class="s1">states + constants)</span>

                <span class="s2"># tf.where needs its condition tensor</span>
                # to be the same shape as its two
                # result tensors, but in our case
                # the condition (mask) tensor is
                # (nsamples, 1), and A and B are (nsamples, ndimensions).
                # So we need to
                # broadcast the mask to match the shape of A and B.
                # That's what the tile call does,
                # it just repeats the mask along its second dimension
                # n times.
                <span class="s1">tiled_mask_t = tf.tile(mask_t</span><span class="s0">,</span>
                                       <span class="s1">tf.stack([</span><span class="s5">1</span><span class="s0">, </span><span class="s1">tf.shape(output)[</span><span class="s5">1</span><span class="s1">]]))</span>

                <span class="s0">if not </span><span class="s1">successive_outputs:</span>
                    prev_output = zeros_like(output)
                <span class="s0">else</span><span class="s1">:</span>
                    prev_output = successive_outputs[-<span class="s5">1</span><span class="s1">]</span>

                output = tf.where(tiled_mask_t<span class="s0">, </span><span class="s1">output</span><span class="s0">, </span><span class="s1">prev_output)</span>

                return_states = []
                <span class="s0">for </span><span class="s1">state</span><span class="s0">, </span><span class="s1">new_state </span><span class="s0">in </span><span class="s1">zip(states</span><span class="s0">, </span><span class="s1">new_states):</span>
                    <span class="s2"># (see earlier comment for tile explanation)</span>
                    <span class="s1">tiled_mask_t = tf.tile(mask_t</span><span class="s0">,</span>
                                           <span class="s1">tf.stack([</span><span class="s5">1</span><span class="s0">, </span><span class="s1">tf.shape(new_state)[</span><span class="s5">1</span><span class="s1">]]))</span>
                    return_states.append(tf.where(tiled_mask_t<span class="s0">,</span>
                                                  <span class="s1">new_state</span><span class="s0">,</span>
                                                  <span class="s1">state))</span>
                states = return_states
                successive_outputs.append(output)
                successive_states.append(states)
            last_output = successive_outputs[-<span class="s5">1</span><span class="s1">]</span>
            new_states = successive_states[-<span class="s5">1</span><span class="s1">]</span>
            outputs = tf.stack(successive_outputs)
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">for </span><span class="s1">inp </span><span class="s0">in </span><span class="s1">input_list:</span>
                output<span class="s0">, </span><span class="s1">states = step_function(inp</span><span class="s0">, </span><span class="s1">states + constants)</span>
                successive_outputs.append(output)
                successive_states.append(states)
            last_output = successive_outputs[-<span class="s5">1</span><span class="s1">]</span>
            new_states = successive_states[-<span class="s5">1</span><span class="s1">]</span>
            outputs = tf.stack(successive_outputs)

    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">go_backwards:</span>
            inputs = reverse(inputs<span class="s0">, </span><span class="s5">0</span><span class="s1">)</span>

        states = tuple(initial_states)

        time_steps = tf.shape(inputs)[<span class="s5">0</span><span class="s1">]</span>
        outputs<span class="s0">, </span><span class="s1">_ = step_function(inputs[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">initial_states + constants)</span>
        output_ta = tensor_array_ops.TensorArray(
            dtype=outputs.dtype<span class="s0">,</span>
            <span class="s1">size=time_steps</span><span class="s0">,</span>
            <span class="s1">tensor_array_name=</span><span class="s3">'output_ta'</span><span class="s1">)</span>
        input_ta = tensor_array_ops.TensorArray(
            dtype=inputs.dtype<span class="s0">,</span>
            <span class="s1">size=time_steps</span><span class="s0">,</span>
            <span class="s1">tensor_array_name=</span><span class="s3">'input_ta'</span><span class="s1">)</span>
        input_ta = input_ta.unstack(inputs)
        time = tf.constant(<span class="s5">0</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s3">'int32'</span><span class="s0">, </span><span class="s1">name=</span><span class="s3">'time'</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">mask </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s0">if not </span><span class="s1">states:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'No initial states provided! '</span>
                                 'When using masking in an RNN, you should '
                                 'provide initial states '
                                 '(and your step function should return '
                                 'as its first state at time `t` '
                                 'the output at time `t-1`).'<span class="s1">)</span>
            <span class="s0">if </span><span class="s1">go_backwards:</span>
                mask = reverse(mask<span class="s0">, </span><span class="s5">0</span><span class="s1">)</span>

            mask_ta = tensor_array_ops.TensorArray(
                dtype=tf.bool<span class="s0">,</span>
                <span class="s1">size=time_steps</span><span class="s0">,</span>
                <span class="s1">tensor_array_name=</span><span class="s3">'mask_ta'</span><span class="s1">)</span>
            mask_ta = mask_ta.unstack(mask)

            <span class="s0">def </span><span class="s1">_step(time</span><span class="s0">, </span><span class="s1">output_ta_t</span><span class="s0">, </span><span class="s1">*states):</span>
                <span class="s4">&quot;&quot;&quot;RNN step function. 
 
                # Arguments 
                    time: Current timestep value. 
                    output_ta_t: TensorArray. 
                    *states: List of states. 
 
                # Returns 
                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)` 
                &quot;&quot;&quot;</span>
                <span class="s1">current_input = input_ta.read(time)</span>
                mask_t = mask_ta.read(time)
                output<span class="s0">, </span><span class="s1">new_states = step_function(current_input</span><span class="s0">,</span>
                                                   <span class="s1">tuple(states) +</span>
                                                   tuple(constants))
                <span class="s0">for </span><span class="s1">state</span><span class="s0">, </span><span class="s1">new_state </span><span class="s0">in </span><span class="s1">zip(states</span><span class="s0">, </span><span class="s1">new_states):</span>
                    new_state.set_shape(state.get_shape())
                tiled_mask_t = tf.tile(mask_t<span class="s0">,</span>
                                       <span class="s1">tf.stack([</span><span class="s5">1</span><span class="s0">, </span><span class="s1">tf.shape(output)[</span><span class="s5">1</span><span class="s1">]]))</span>
                output = tf.where(tiled_mask_t<span class="s0">, </span><span class="s1">output</span><span class="s0">, </span><span class="s1">states[</span><span class="s5">0</span><span class="s1">])</span>
                new_states = [tf.where(tiled_mask_t<span class="s0">, </span><span class="s1">new_states[i]</span><span class="s0">, </span><span class="s1">states[i]) </span><span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(len(states))]</span>
                output_ta_t = output_ta_t.write(time<span class="s0">, </span><span class="s1">output)</span>
                <span class="s0">return </span><span class="s1">(time + </span><span class="s5">1</span><span class="s0">, </span><span class="s1">output_ta_t) + tuple(new_states)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">def </span><span class="s1">_step(time</span><span class="s0">, </span><span class="s1">output_ta_t</span><span class="s0">, </span><span class="s1">*states):</span>
                <span class="s4">&quot;&quot;&quot;RNN step function. 
 
                # Arguments 
                    time: Current timestep value. 
                    output_ta_t: TensorArray. 
                    *states: List of states. 
 
                # Returns 
                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)` 
                &quot;&quot;&quot;</span>
                <span class="s1">current_input = input_ta.read(time)</span>
                output<span class="s0">, </span><span class="s1">new_states = step_function(current_input</span><span class="s0">,</span>
                                                   <span class="s1">tuple(states) +</span>
                                                   tuple(constants))
                <span class="s0">for </span><span class="s1">state</span><span class="s0">, </span><span class="s1">new_state </span><span class="s0">in </span><span class="s1">zip(states</span><span class="s0">, </span><span class="s1">new_states):</span>
                    new_state.set_shape(state.get_shape())
                output_ta_t = output_ta_t.write(time<span class="s0">, </span><span class="s1">output)</span>
                <span class="s0">return </span><span class="s1">(time + </span><span class="s5">1</span><span class="s0">, </span><span class="s1">output_ta_t) + tuple(new_states)</span>

        final_outputs = control_flow_ops.while_loop(
            cond=<span class="s0">lambda </span><span class="s1">time</span><span class="s0">, </span><span class="s1">*_: time &lt; time_steps</span><span class="s0">,</span>
            <span class="s1">body=_step</span><span class="s0">,</span>
            <span class="s1">loop_vars=(time</span><span class="s0">, </span><span class="s1">output_ta) + states</span><span class="s0">,</span>
            <span class="s1">parallel_iterations=</span><span class="s5">32</span><span class="s0">,</span>
            <span class="s1">swap_memory=</span><span class="s0">True</span><span class="s1">)</span>
        last_time = final_outputs[<span class="s5">0</span><span class="s1">]</span>
        output_ta = final_outputs[<span class="s5">1</span><span class="s1">]</span>
        new_states = final_outputs[<span class="s5">2</span><span class="s1">:]</span>

        outputs = output_ta.stack()
        last_output = output_ta.read(last_time - <span class="s5">1</span><span class="s1">)</span>

    axes = [<span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">] + list(range(</span><span class="s5">2</span><span class="s0">, </span><span class="s1">len(outputs.get_shape())))</span>
    outputs = tf.transpose(outputs<span class="s0">, </span><span class="s1">axes)</span>
    <span class="s0">return </span><span class="s1">last_output</span><span class="s0">, </span><span class="s1">outputs</span><span class="s0">, </span><span class="s1">new_states</span>


<span class="s0">def </span><span class="s1">switch(condition</span><span class="s0">, </span><span class="s1">then_expression</span><span class="s0">, </span><span class="s1">else_expression):</span>
    <span class="s4">&quot;&quot;&quot;Switches between two operations depending on a scalar value. 
 
    Note that both `then_expression` and `else_expression` 
    should be symbolic tensors of the *same shape*. 
 
    # Arguments 
        condition: scalar tensor (`int` or `bool`). 
        then_expression: either a tensor, or a callable that returns a tensor. 
        else_expression: either a tensor, or a callable that returns a tensor. 
 
    # Returns 
        The selected tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">condition.dtype != tf.bool:</span>
        condition = tf.cast(condition<span class="s0">, </span><span class="s3">'bool'</span><span class="s1">)</span>
    <span class="s0">if not </span><span class="s1">callable(then_expression):</span>
        <span class="s0">def </span><span class="s1">then_expression_fn():</span>
            <span class="s0">return </span><span class="s1">then_expression</span>
    <span class="s0">else</span><span class="s1">:</span>
        then_expression_fn = then_expression
    <span class="s0">if not </span><span class="s1">callable(else_expression):</span>
        <span class="s0">def </span><span class="s1">else_expression_fn():</span>
            <span class="s0">return </span><span class="s1">else_expression</span>
    <span class="s0">else</span><span class="s1">:</span>
        else_expression_fn = else_expression
    x = tf.cond(condition<span class="s0">,</span>
                <span class="s1">then_expression_fn</span><span class="s0">,</span>
                <span class="s1">else_expression_fn)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">in_train_phase(x</span><span class="s0">, </span><span class="s1">alt</span><span class="s0">, </span><span class="s1">training=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Selects `x` in train phase, and `alt` otherwise. 
 
    Note that `alt` should have the *same shape* as `x`. 
 
    # Arguments 
        x: What to return in train phase 
            (tensor or callable that returns a tensor). 
        alt: What to return otherwise 
            (tensor or callable that returns a tensor). 
        training: Optional scalar tensor 
            (or Python boolean, or Python integer) 
            specifing the learning phase. 
 
    # Returns 
        Either `x` or `alt` based on the `training` flag. 
        the `training` flag defaults to `K.learning_phase()`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">training </span><span class="s0">is None</span><span class="s1">:</span>
        training = learning_phase()
        uses_learning_phase = <span class="s0">True</span>
    else<span class="s1">:</span>
        uses_learning_phase = <span class="s0">False</span>

    if <span class="s1">training </span><span class="s0">is </span><span class="s5">1 </span><span class="s0">or </span><span class="s1">training </span><span class="s0">is True</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">callable(x):</span>
            <span class="s0">return </span><span class="s1">x()</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">elif </span><span class="s1">training </span><span class="s0">is </span><span class="s5">0 </span><span class="s0">or </span><span class="s1">training </span><span class="s0">is False</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">callable(alt):</span>
            <span class="s0">return </span><span class="s1">alt()</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">alt</span>

    <span class="s2"># else: assume learning phase is a placeholder tensor.</span>
    <span class="s1">x = switch(training</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">alt)</span>
    <span class="s0">if </span><span class="s1">uses_learning_phase:</span>
        x._uses_learning_phase = <span class="s0">True</span>
    return <span class="s1">x</span>


<span class="s0">def </span><span class="s1">in_test_phase(x</span><span class="s0">, </span><span class="s1">alt</span><span class="s0">, </span><span class="s1">training=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Selects `x` in test phase, and `alt` otherwise. 
 
    Note that `alt` should have the *same shape* as `x`. 
 
    # Arguments 
        x: What to return in test phase 
            (tensor or callable that returns a tensor). 
        alt: What to return otherwise 
            (tensor or callable that returns a tensor). 
        training: Optional scalar tensor 
            (or Python boolean, or Python integer) 
            specifing the learning phase. 
 
    # Returns 
        Either `x` or `alt` based on `K.learning_phase`. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">in_train_phase(alt</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">training=training)</span>


<span class="s2"># NN OPERATIONS</span>

<span class="s0">def </span><span class="s1">relu(x</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s5">0.</span><span class="s0">, </span><span class="s1">max_value=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Rectified linear unit. 
 
    With default values, it returns element-wise `max(x, 0)`. 
 
    # Arguments 
        x: A tensor or variable. 
        alpha: A scalar, slope of negative section (default=`0.`). 
        max_value: Saturation threshold. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">alpha != </span><span class="s5">0.</span><span class="s1">:</span>
        negative_part = tf.nn.relu(-x)
    x = tf.nn.relu(x)
    <span class="s0">if </span><span class="s1">max_value </span><span class="s0">is not None</span><span class="s1">:</span>
        max_value = _to_tensor(max_value<span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
        zero = _to_tensor(<span class="s5">0.</span><span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
        x = tf.clip_by_value(x<span class="s0">, </span><span class="s1">zero</span><span class="s0">, </span><span class="s1">max_value)</span>
    <span class="s0">if </span><span class="s1">alpha != </span><span class="s5">0.</span><span class="s1">:</span>
        alpha = _to_tensor(alpha<span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
        x -= alpha * negative_part
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">elu(x</span><span class="s0">, </span><span class="s1">alpha=</span><span class="s5">1.</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Exponential linear unit. 
 
    # Arguments 
        x: A tenor or variable to compute the activation function for. 
        alpha: A scalar, slope of positive section. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">res = tf.nn.elu(x)</span>
    <span class="s0">if </span><span class="s1">alpha == </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">res</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.where(x &gt; </span><span class="s5">0</span><span class="s0">, </span><span class="s1">res</span><span class="s0">, </span><span class="s1">alpha * res)</span>


<span class="s0">def </span><span class="s1">softmax(x):</span>
    <span class="s4">&quot;&quot;&quot;Softmax of a tensor. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.softmax(x)</span>


<span class="s0">def </span><span class="s1">softplus(x):</span>
    <span class="s4">&quot;&quot;&quot;Softplus of a tensor. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.softplus(x)</span>


<span class="s0">def </span><span class="s1">softsign(x):</span>
    <span class="s4">&quot;&quot;&quot;Softsign of a tensor. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.softsign(x)</span>


<span class="s0">def </span><span class="s1">categorical_crossentropy(output</span><span class="s0">, </span><span class="s1">target</span><span class="s0">, </span><span class="s1">from_logits=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Categorical crossentropy between an output tensor and a target tensor. 
 
    # Arguments 
        output: A tensor resulting from a softmax 
            (unless `from_logits` is True, in which 
            case `output` is expected to be the logits). 
        target: A tensor of the same shape as `output`. 
        from_logits: Boolean, whether `output` is the 
            result of a softmax, or is a tensor of logits. 
 
    # Returns 
        Output tensor. 
    &quot;&quot;&quot;</span>
    <span class="s2"># Note: tf.nn.softmax_cross_entropy_with_logits</span>
    # expects logits, Keras expects probabilities.
    <span class="s0">if not </span><span class="s1">from_logits:</span>
        <span class="s2"># scale preds so that the class probas of each sample sum to 1</span>
        <span class="s1">output /= tf.reduce_sum(output</span><span class="s0">,</span>
                                <span class="s1">reduction_indices=len(output.get_shape()) - </span><span class="s5">1</span><span class="s0">,</span>
                                <span class="s1">keep_dims=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s2"># manual computation of crossentropy</span>
        <span class="s1">epsilon = _to_tensor(_EPSILON</span><span class="s0">, </span><span class="s1">output.dtype.base_dtype)</span>
        output = tf.clip_by_value(output<span class="s0">, </span><span class="s1">epsilon</span><span class="s0">, </span><span class="s5">1. </span><span class="s1">- epsilon)</span>
        <span class="s0">return </span><span class="s1">- tf.reduce_sum(target * tf.log(output)</span><span class="s0">,</span>
                               <span class="s1">reduction_indices=len(output.get_shape()) - </span><span class="s5">1</span><span class="s1">)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">tf.nn.softmax_cross_entropy_with_logits(labels=target</span><span class="s0">,</span>
                                                       <span class="s1">logits=output)</span>


<span class="s0">def </span><span class="s1">sparse_categorical_crossentropy(output</span><span class="s0">, </span><span class="s1">target</span><span class="s0">, </span><span class="s1">from_logits=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Categorical crossentropy with integer targets. 
 
    # Arguments 
        output: A tensor resulting from a softmax 
            (unless `from_logits` is True, in which 
            case `output` is expected to be the logits). 
        target: An integer tensor. 
        from_logits: Boolean, whether `output` is the 
            result of a softmax, or is a tensor of logits. 
 
    # Returns 
        Output tensor. 
    &quot;&quot;&quot;</span>
    <span class="s2"># Note: tf.nn.sparse_softmax_cross_entropy_with_logits</span>
    # expects logits, Keras expects probabilities.
    <span class="s0">if not </span><span class="s1">from_logits:</span>
        epsilon = _to_tensor(_EPSILON<span class="s0">, </span><span class="s1">output.dtype.base_dtype)</span>
        output = tf.clip_by_value(output<span class="s0">, </span><span class="s1">epsilon</span><span class="s0">, </span><span class="s5">1 </span><span class="s1">- epsilon)</span>
        output = tf.log(output)

    output_shape = output.get_shape()
    targets = cast(flatten(target)<span class="s0">, </span><span class="s3">'int64'</span><span class="s1">)</span>
    logits = tf.reshape(output<span class="s0">, </span><span class="s1">[-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">int(output_shape[-</span><span class="s5">1</span><span class="s1">])])</span>
    res = tf.nn.sparse_softmax_cross_entropy_with_logits(
        labels=targets<span class="s0">,</span>
        <span class="s1">logits=logits)</span>
    <span class="s0">if </span><span class="s1">len(output_shape) == </span><span class="s5">3</span><span class="s1">:</span>
        <span class="s2"># if our output includes timesteps we need to reshape</span>
        <span class="s0">return </span><span class="s1">tf.reshape(res</span><span class="s0">, </span><span class="s1">tf.shape(output)[:-</span><span class="s5">1</span><span class="s1">])</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">return </span><span class="s1">res</span>


<span class="s0">def </span><span class="s1">binary_crossentropy(output</span><span class="s0">, </span><span class="s1">target</span><span class="s0">, </span><span class="s1">from_logits=</span><span class="s0">False</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor. 
 
    # Arguments 
        output: A tensor. 
        target: A tensor with the same shape as `output`. 
        from_logits: Whether `output` is expected to be a logits tensor. 
            By default, we consider that `output` 
            encodes a probability distribution. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s2"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span>
    # expects logits, Keras expects probabilities.
    <span class="s0">if not </span><span class="s1">from_logits:</span>
        <span class="s2"># transform back to logits</span>
        <span class="s1">epsilon = _to_tensor(_EPSILON</span><span class="s0">, </span><span class="s1">output.dtype.base_dtype)</span>
        output = tf.clip_by_value(output<span class="s0">, </span><span class="s1">epsilon</span><span class="s0">, </span><span class="s5">1 </span><span class="s1">- epsilon)</span>
        output = tf.log(output / (<span class="s5">1 </span><span class="s1">- output))</span>

    <span class="s0">return </span><span class="s1">tf.nn.sigmoid_cross_entropy_with_logits(labels=target</span><span class="s0">,</span>
                                                   <span class="s1">logits=output)</span>


<span class="s0">def </span><span class="s1">sigmoid(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise sigmoid. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.sigmoid(x)</span>


<span class="s0">def </span><span class="s1">hard_sigmoid(x):</span>
    <span class="s4">&quot;&quot;&quot;Segment-wise linear approximation of sigmoid. 
 
    Faster than sigmoid. 
    Returns `0.` if `x &lt; -2.5`, `1.` if `x &gt; 2.5`. 
    In `-2.5 &lt;= x &lt;= 2.5`, returns `0.2 * x + 0.5`. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">x = (</span><span class="s5">0.2 </span><span class="s1">* x) + </span><span class="s5">0.5</span>
    <span class="s1">zero = _to_tensor(</span><span class="s5">0.</span><span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
    one = _to_tensor(<span class="s5">1.</span><span class="s0">, </span><span class="s1">x.dtype.base_dtype)</span>
    x = tf.clip_by_value(x<span class="s0">, </span><span class="s1">zero</span><span class="s0">, </span><span class="s1">one)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">tanh(x):</span>
    <span class="s4">&quot;&quot;&quot;Element-wise tanh. 
 
    # Arguments 
        x: A tensor or variable. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.tanh(x)</span>


<span class="s0">def </span><span class="s1">dropout(x</span><span class="s0">, </span><span class="s1">level</span><span class="s0">, </span><span class="s1">noise_shape=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Sets entries in `x` to zero at random, while scaling the entire tensor. 
 
    # Arguments 
        x: tensor 
        level: fraction of the entries in the tensor 
            that will be set to 0. 
        noise_shape: shape for randomly generated keep/drop flags, 
            must be broadcastable to the shape of `x` 
        seed: random seed to ensure determinism. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">retain_prob = </span><span class="s5">1. </span><span class="s1">- level</span>
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        seed = np.random.randint(<span class="s5">10e6</span><span class="s1">)</span>
    <span class="s2"># the dummy 1. works around a TF bug</span>
    # (float32_ref vs. float32 incompatibility)
    <span class="s0">return </span><span class="s1">tf.nn.dropout(x * </span><span class="s5">1.</span><span class="s0">, </span><span class="s1">retain_prob</span><span class="s0">, </span><span class="s1">noise_shape</span><span class="s0">, </span><span class="s1">seed=seed)</span>


<span class="s0">def </span><span class="s1">l2_normalize(x</span><span class="s0">, </span><span class="s1">axis):</span>
    <span class="s4">&quot;&quot;&quot;Normalizes a tensor wrt the L2 norm alongside the specified axis. 
 
    # Arguments 
        x: Tensor or variable. 
        axis: axis along which to perform normalization. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">axis &lt; </span><span class="s5">0</span><span class="s1">:</span>
        axis %= len(x.get_shape())
    <span class="s0">return </span><span class="s1">tf.nn.l2_normalize(x</span><span class="s0">, </span><span class="s1">dim=axis)</span>


<span class="s0">def </span><span class="s1">in_top_k(predictions</span><span class="s0">, </span><span class="s1">targets</span><span class="s0">, </span><span class="s1">k):</span>
    <span class="s4">&quot;&quot;&quot;Returns whether the `targets` are in the top `k` `predictions`. 
 
    # Arguments 
        predictions: A tensor of shape `(batch_size, classes)` and type `float32`. 
        targets: A 1D tensor of length `batch_size` and type `int32` or `int64`. 
        k: An `int`, number of top elements to consider. 
 
    # Returns 
        A 1D tensor of length `batch_size` and type `bool`. 
        `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k` 
        values of `predictions[i]`. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.nn.in_top_k(predictions</span><span class="s0">, </span><span class="s1">targets</span><span class="s0">, </span><span class="s1">k)</span>


<span class="s2"># CONVOLUTIONS</span>

<span class="s0">def </span><span class="s1">_preprocess_deconv_output_shape(x</span><span class="s0">, </span><span class="s1">shape</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Get the output_shape for the deconvolution. 
 
    # Arguments 
        x: input tensor. 
        shape: output shape. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        The output shape. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        shape = (shape[<span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">shape[</span><span class="s5">2</span><span class="s1">]</span><span class="s0">, </span><span class="s1">shape[</span><span class="s5">3</span><span class="s1">]</span><span class="s0">, </span><span class="s1">shape[</span><span class="s5">1</span><span class="s1">])</span>

    <span class="s0">if </span><span class="s1">shape[</span><span class="s5">0</span><span class="s1">] </span><span class="s0">is None</span><span class="s1">:</span>
        shape = (tf.shape(x)[<span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s1">) + tuple(shape[</span><span class="s5">1</span><span class="s1">:])</span>
        shape = tf.stack(list(shape))
    <span class="s0">return </span><span class="s1">shape</span>


<span class="s0">def </span><span class="s1">_preprocess_conv2d_input(x</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Transpose and cast the input before the conv2d. 
 
    # Arguments 
        x: input tensor. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype(x) == </span><span class="s3">'float64'</span><span class="s1">:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s3">'float32'</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        <span class="s2"># TF uses the last dimension as channel dimension,</span>
        # instead of the 2nd one.
        # TH input shape: (samples, input_depth, rows, cols)
        # TF input shape: (samples, rows, cols, input_depth)
        <span class="s1">x = tf.transpose(x</span><span class="s0">, </span><span class="s1">(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">_preprocess_conv3d_input(x</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Transpose and cast the input before the conv3d. 
 
    # Arguments 
        x: input tensor. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype(x) == </span><span class="s3">'float64'</span><span class="s1">:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s3">'float32'</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        x = tf.transpose(x<span class="s0">, </span><span class="s1">(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">_preprocess_conv2d_kernel(kernel</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Transpose and cast the kernel before the conv2d. 
 
    # Arguments 
        kernel: kernel tensor. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype(kernel) == </span><span class="s3">'float64'</span><span class="s1">:</span>
        kernel = tf.cast(kernel<span class="s0">, </span><span class="s3">'float32'</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        kernel = tf.transpose(kernel<span class="s0">, </span><span class="s1">(</span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">kernel</span>


<span class="s0">def </span><span class="s1">_preprocess_conv3d_kernel(kernel</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Transpose and cast the kernel before the conv3d. 
 
    # Arguments 
        kernel: kernel tensor. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype(kernel) == </span><span class="s3">'float64'</span><span class="s1">:</span>
        kernel = tf.cast(kernel<span class="s0">, </span><span class="s3">'float32'</span><span class="s1">)</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        kernel = tf.transpose(kernel<span class="s0">, </span><span class="s1">(</span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">kernel</span>


<span class="s0">def </span><span class="s1">_preprocess_padding(padding):</span>
    <span class="s4">&quot;&quot;&quot;Convert keras' padding to tensorflow's padding. 
 
    # Arguments 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
 
    # Returns 
        a string, `&quot;SAME&quot;` or `&quot;VALID&quot;`. 
 
    # Raises 
        ValueError: if `padding` is invalid. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">padding == </span><span class="s3">'same'</span><span class="s1">:</span>
        padding = <span class="s3">'SAME'</span>
    <span class="s0">elif </span><span class="s1">padding == </span><span class="s3">'valid'</span><span class="s1">:</span>
        padding = <span class="s3">'VALID'</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Invalid padding:'</span><span class="s0">, </span><span class="s1">padding)</span>
    <span class="s0">return </span><span class="s1">padding</span>


<span class="s0">def </span><span class="s1">_postprocess_conv2d_output(x</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Transpose and cast the output from conv2d if needed. 
 
    # Arguments 
        x: A tensor. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>

    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        x = tf.transpose(x<span class="s0">, </span><span class="s1">(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s1">))</span>

    <span class="s0">if </span><span class="s1">floatx() == </span><span class="s3">'float64'</span><span class="s1">:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s3">'float64'</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">_postprocess_conv3d_output(x</span><span class="s0">, </span><span class="s1">data_format):</span>
    <span class="s4">&quot;&quot;&quot;Transpose and cast the output from conv3d if needed. 
 
    # Arguments 
        x: A tensor. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        x = tf.transpose(x<span class="s0">, </span><span class="s1">(</span><span class="s5">0</span><span class="s0">, </span><span class="s5">4</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s1">))</span>

    <span class="s0">if </span><span class="s1">floatx() == </span><span class="s3">'float64'</span><span class="s1">:</span>
        x = tf.cast(x<span class="s0">, </span><span class="s3">'float64'</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">conv1d(x</span><span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">strides=</span><span class="s5">1</span><span class="s0">, </span><span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">,</span>
           <span class="s1">data_format=</span><span class="s0">None, </span><span class="s1">dilation_rate=</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;1D convolution. 
 
    # Arguments 
        x: Tensor or variable. 
        kernel: kernel tensor. 
        strides: stride integer. 
        padding: string, `&quot;same&quot;`, `&quot;causal&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
        dilation_rate: integer dilate rate. 
 
    # Returns 
        A tensor, result of 1D convolution. 
    &quot;&quot;&quot;</span>
    <span class="s1">kernel_shape = kernel.get_shape().as_list()</span>
    <span class="s0">if </span><span class="s1">padding == </span><span class="s3">'causal'</span><span class="s1">:</span>
        <span class="s2"># causal (dilated) convolution:</span>
        <span class="s1">left_pad = dilation_rate * (kernel_shape[</span><span class="s5">0</span><span class="s1">] - </span><span class="s5">1</span><span class="s1">)</span>
        x = temporal_padding(x<span class="s0">, </span><span class="s1">(left_pad</span><span class="s0">, </span><span class="s5">0</span><span class="s1">))</span>
        padding = <span class="s3">'valid'</span>
    <span class="s1">padding = _preprocess_padding(padding)</span>
    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_last'</span><span class="s1">:</span>
        tf_data_format = <span class="s3">'NWC'</span>
    <span class="s0">else</span><span class="s1">:</span>
        tf_data_format = <span class="s3">'NCW'</span>
    <span class="s1">x = tf.nn.convolution(</span>
        input=x<span class="s0">,</span>
        <span class="s1">filter=kernel</span><span class="s0">,</span>
        <span class="s1">dilation_rate=(dilation_rate</span><span class="s0">,</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">strides=(strides</span><span class="s0">,</span><span class="s1">)</span><span class="s0">,</span>
        <span class="s1">padding=padding</span><span class="s0">,</span>
        <span class="s1">data_format=tf_data_format)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">conv2d(x</span><span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">strides=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">,</span>
           <span class="s1">data_format=</span><span class="s0">None, </span><span class="s1">dilation_rate=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)):</span>
    <span class="s4">&quot;&quot;&quot;2D convolution. 
 
    # Arguments 
        x: Tensor or variable. 
        kernel: kernel tensor. 
        strides: strides tuple. 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
            Whether to use Theano or TensorFlow data format 
            for inputs/kernels/outputs. 
        dilation_rate: tuple of 2 integers. 
 
    # Returns 
        A tensor, result of 2D convolution. 
 
    # Raises 
        ValueError: if `data_format` is neither `channels_last` or `channels_first`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    <span class="s2"># With 4d inputs, tf.nn.convolution only supports</span>
    # data_format NHWC, so we transpose the inputs
    # in case we are in data_format channels_first.
    <span class="s1">x = _preprocess_conv2d_input(x</span><span class="s0">, </span><span class="s1">data_format)</span>
    padding = _preprocess_padding(padding)
    x = tf.nn.convolution(
        input=x<span class="s0">,</span>
        <span class="s1">filter=kernel</span><span class="s0">,</span>
        <span class="s1">dilation_rate=dilation_rate</span><span class="s0">,</span>
        <span class="s1">strides=strides</span><span class="s0">,</span>
        <span class="s1">padding=padding</span><span class="s0">,</span>
        <span class="s1">data_format=</span><span class="s3">'NHWC'</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">_postprocess_conv2d_output(x</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">conv2d_transpose(x</span><span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">output_shape</span><span class="s0">, </span><span class="s1">strides=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,</span>
                     <span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;2D deconvolution (i.e. transposed convolution). 
 
    # Arguments 
        x: Tensor or variable. 
        kernel: kernel tensor. 
        output_shape: 1D int tensor for the output shape. 
        strides: strides tuple. 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
            Whether to use Theano or TensorFlow data format 
            for inputs/kernels/outputs. 
 
    # Returns 
        A tensor, result of transposed 2D convolution. 
 
    # Raises 
        ValueError: if `data_format` is neither `channels_last` or `channels_first`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>
    <span class="s0">if </span><span class="s1">isinstance(output_shape</span><span class="s0">, </span><span class="s1">(tuple</span><span class="s0">, </span><span class="s1">list)):</span>
        output_shape = tf.stack(output_shape)

    x = _preprocess_conv2d_input(x<span class="s0">, </span><span class="s1">data_format)</span>
    output_shape = _preprocess_deconv_output_shape(x<span class="s0">, </span><span class="s1">output_shape</span><span class="s0">, </span><span class="s1">data_format)</span>
    padding = _preprocess_padding(padding)
    strides = (<span class="s5">1</span><span class="s0">,</span><span class="s1">) + strides + (</span><span class="s5">1</span><span class="s0">,</span><span class="s1">)</span>

    x = tf.nn.conv2d_transpose(x<span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">output_shape</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">,</span>
                               <span class="s1">padding=padding)</span>
    x = _postprocess_conv2d_output(x<span class="s0">, </span><span class="s1">data_format)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">def </span><span class="s1">separable_conv2d(x</span><span class="s0">, </span><span class="s1">depthwise_kernel</span><span class="s0">, </span><span class="s1">pointwise_kernel</span><span class="s0">, </span><span class="s1">strides=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,</span>
                     <span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None, </span><span class="s1">dilation_rate=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)):</span>
    <span class="s4">&quot;&quot;&quot;2D convolution with separable filters. 
 
    # Arguments 
        x: input tensor 
        depthwise_kernel: convolution kernel for the depthwise convolution. 
        pointwise_kernel: kernel for the 1x1 convolution. 
        strides: strides tuple (length 2). 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
        dilation_rate: tuple of integers, 
            dilation rates for the separable convolution. 
 
    # Returns 
        Output tensor. 
 
    # Raises 
        ValueError: if `data_format` is neither `channels_last` or `channels_first`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    x = _preprocess_conv2d_input(x<span class="s0">, </span><span class="s1">data_format)</span>
    padding = _preprocess_padding(padding)
    strides = (<span class="s5">1</span><span class="s0">,</span><span class="s1">) + strides + (</span><span class="s5">1</span><span class="s0">,</span><span class="s1">)</span>

    x = tf.nn.separable_conv2d(x<span class="s0">, </span><span class="s1">depthwise_kernel</span><span class="s0">, </span><span class="s1">pointwise_kernel</span><span class="s0">,</span>
                               <span class="s1">strides=strides</span><span class="s0">,</span>
                               <span class="s1">padding=padding</span><span class="s0">,</span>
                               <span class="s1">rate=dilation_rate)</span>
    <span class="s0">return </span><span class="s1">_postprocess_conv2d_output(x</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">conv3d(x</span><span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">strides=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">,</span>
           <span class="s1">data_format=</span><span class="s0">None, </span><span class="s1">dilation_rate=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)):</span>
    <span class="s4">&quot;&quot;&quot;3D convolution. 
 
    # Arguments 
        x: Tensor or variable. 
        kernel: kernel tensor. 
        strides: strides tuple. 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
            Whether to use Theano or TensorFlow data format 
            for inputs/kernels/outputs. 
        dilation_rate: tuple of 3 integers. 
 
    # Returns 
        A tensor, result of 3D convolution. 
 
    # Raises 
        ValueError: if `data_format` is neither `channels_last` or `channels_first`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    <span class="s2"># With 5d inputs, tf.nn.convolution only supports</span>
    # data_format NDHWC, so we transpose the inputs
    # in case we are in data_format channels_first.
    <span class="s1">x = _preprocess_conv3d_input(x</span><span class="s0">, </span><span class="s1">data_format)</span>
    padding = _preprocess_padding(padding)
    x = tf.nn.convolution(
        input=x<span class="s0">,</span>
        <span class="s1">filter=kernel</span><span class="s0">,</span>
        <span class="s1">dilation_rate=dilation_rate</span><span class="s0">,</span>
        <span class="s1">strides=strides</span><span class="s0">,</span>
        <span class="s1">padding=padding</span><span class="s0">,</span>
        <span class="s1">data_format=</span><span class="s3">'NDHWC'</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">_postprocess_conv3d_output(x</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">pool2d(x</span><span class="s0">, </span><span class="s1">pool_size</span><span class="s0">, </span><span class="s1">strides=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">,</span>
           <span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None,</span>
           <span class="s1">pool_mode=</span><span class="s3">'max'</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;2D Pooling. 
 
    # Arguments 
        x: Tensor or variable. 
        pool_size: tuple of 2 integers. 
        strides: tuple of 2 integers. 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
        pool_mode: string, `&quot;max&quot;` or `&quot;avg&quot;`. 
 
    # Returns 
        A tensor, result of 2D pooling. 
 
    # Raises 
        ValueError: if `data_format` is neither `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
        ValueError: if `pool_mode` is neither `&quot;max&quot;` or `&quot;avg&quot;`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    padding = _preprocess_padding(padding)
    strides = (<span class="s5">1</span><span class="s0">,</span><span class="s1">) + strides + (</span><span class="s5">1</span><span class="s0">,</span><span class="s1">)</span>
    pool_size = (<span class="s5">1</span><span class="s0">,</span><span class="s1">) + pool_size + (</span><span class="s5">1</span><span class="s0">,</span><span class="s1">)</span>

    x = _preprocess_conv2d_input(x<span class="s0">, </span><span class="s1">data_format)</span>

    <span class="s0">if </span><span class="s1">pool_mode == </span><span class="s3">'max'</span><span class="s1">:</span>
        x = tf.nn.max_pool(x<span class="s0">, </span><span class="s1">pool_size</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">, </span><span class="s1">padding=padding)</span>
    <span class="s0">elif </span><span class="s1">pool_mode == </span><span class="s3">'avg'</span><span class="s1">:</span>
        x = tf.nn.avg_pool(x<span class="s0">, </span><span class="s1">pool_size</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">, </span><span class="s1">padding=padding)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Invalid pooling mode:'</span><span class="s0">, </span><span class="s1">pool_mode)</span>

    <span class="s0">return </span><span class="s1">_postprocess_conv2d_output(x</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">pool3d(x</span><span class="s0">, </span><span class="s1">pool_size</span><span class="s0">, </span><span class="s1">strides=(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span><span class="s0">, </span><span class="s1">padding=</span><span class="s3">'valid'</span><span class="s0">,</span>
           <span class="s1">data_format=</span><span class="s0">None, </span><span class="s1">pool_mode=</span><span class="s3">'max'</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;3D Pooling. 
 
    # Arguments 
        x: Tensor or variable. 
        pool_size: tuple of 3 integers. 
        strides: tuple of 3 integers. 
        padding: string, `&quot;same&quot;` or `&quot;valid&quot;`. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
        pool_mode: string, `&quot;max&quot;` or `&quot;avg&quot;`. 
 
    # Returns 
        A tensor, result of 3D pooling. 
 
    # Raises 
        ValueError: if `data_format` is neither `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
        ValueError: if `pool_mode` is neither `&quot;max&quot;` or `&quot;avg&quot;`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    padding = _preprocess_padding(padding)
    strides = (<span class="s5">1</span><span class="s0">,</span><span class="s1">) + strides + (</span><span class="s5">1</span><span class="s0">,</span><span class="s1">)</span>
    pool_size = (<span class="s5">1</span><span class="s0">,</span><span class="s1">) + pool_size + (</span><span class="s5">1</span><span class="s0">,</span><span class="s1">)</span>

    x = _preprocess_conv3d_input(x<span class="s0">, </span><span class="s1">data_format)</span>

    <span class="s0">if </span><span class="s1">pool_mode == </span><span class="s3">'max'</span><span class="s1">:</span>
        x = tf.nn.max_pool3d(x<span class="s0">, </span><span class="s1">pool_size</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">, </span><span class="s1">padding=padding)</span>
    <span class="s0">elif </span><span class="s1">pool_mode == </span><span class="s3">'avg'</span><span class="s1">:</span>
        x = tf.nn.avg_pool3d(x<span class="s0">, </span><span class="s1">pool_size</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">, </span><span class="s1">padding=padding)</span>
    <span class="s0">else</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Invalid pooling mode:'</span><span class="s0">, </span><span class="s1">pool_mode)</span>

    <span class="s0">return </span><span class="s1">_postprocess_conv3d_output(x</span><span class="s0">, </span><span class="s1">data_format)</span>


<span class="s0">def </span><span class="s1">bias_add(x</span><span class="s0">, </span><span class="s1">bias</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Adds a bias vector to a tensor. 
 
    # Arguments 
        x: Tensor or variable. 
        bias: Bias tensor to add. 
        data_format: string, `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
 
    # Returns 
        Output tensor. 
 
    # Raises 
        ValueError: In one of the two cases below: 
                    1. invalid `data_format` argument. 
                    2. invalid bias shape. 
                       the bias should be either a vector or 
                       a tensor with ndim(x) - 1 dimension 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>
    bias_shape = int_shape(bias)
    <span class="s0">if </span><span class="s1">len(bias_shape) != </span><span class="s5">1 </span><span class="s0">and </span><span class="s1">len(bias_shape) != ndim(x) - </span><span class="s5">1</span><span class="s1">:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unexpected bias dimensions %d, expect to be 1 or %d dimensions'</span>
                         <span class="s1">% (len(bias_shape)</span><span class="s0">, </span><span class="s1">ndim(x)))</span>
    <span class="s0">if </span><span class="s1">ndim(x) == </span><span class="s5">5</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(bias_shape) == </span><span class="s5">1</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s0">else</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">3</span><span class="s1">]) + bias_shape[:</span><span class="s5">3</span><span class="s1">])</span>
        <span class="s0">elif </span><span class="s1">data_format == </span><span class="s3">'channels_last'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(bias_shape) == </span><span class="s5">1</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">0</span><span class="s1">]))</span>
            <span class="s0">else</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">,</span><span class="s1">) + bias_shape)</span>
    <span class="s0">elif </span><span class="s1">ndim(x) == </span><span class="s5">4</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(bias_shape) == </span><span class="s5">1</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s0">else</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">2</span><span class="s1">]) + bias_shape[:</span><span class="s5">2</span><span class="s1">])</span>
        <span class="s0">elif </span><span class="s1">data_format == </span><span class="s3">'channels_last'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(bias_shape) == </span><span class="s5">1</span><span class="s1">:</span>
                x = tf.nn.bias_add(x<span class="s0">, </span><span class="s1">bias</span><span class="s0">,</span>
                                   <span class="s1">data_format=</span><span class="s3">'NHWC'</span><span class="s1">)</span>
            <span class="s0">else</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">,</span><span class="s1">) + bias_shape)</span>
    <span class="s0">elif </span><span class="s1">ndim(x) == </span><span class="s5">3</span><span class="s1">:</span>
        <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(bias_shape) == </span><span class="s5">1</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">0</span><span class="s1">]</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span>
            <span class="s0">else</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">1</span><span class="s1">]</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">0</span><span class="s1">]))</span>
        <span class="s0">elif </span><span class="s1">data_format == </span><span class="s3">'channels_last'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(bias_shape) == </span><span class="s5">1</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s1">bias_shape[</span><span class="s5">0</span><span class="s1">]))</span>
            <span class="s0">else</span><span class="s1">:</span>
                x += reshape(bias<span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">) + bias_shape)</span>
    <span class="s0">else</span><span class="s1">:</span>
        x = tf.nn.bias_add(x<span class="s0">, </span><span class="s1">bias)</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s2"># RANDOMNESS</span>

<span class="s0">def </span><span class="s1">random_normal(shape</span><span class="s0">, </span><span class="s1">mean=</span><span class="s5">0.0</span><span class="s0">, </span><span class="s1">stddev=</span><span class="s5">1.0</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns a tensor with normal distribution of values. 
 
    # Arguments 
        shape: A tuple of integers, the shape of tensor to create. 
        mean: A float, mean of the normal distribution to draw samples. 
        stddev: A float, standard deviation of the normal distribution 
            to draw samples. 
        dtype: String, dtype of returned tensor. 
        seed: Integer, random seed. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        seed = np.random.randint(<span class="s5">10e6</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">tf.random_normal(shape</span><span class="s0">, </span><span class="s1">mean=mean</span><span class="s0">, </span><span class="s1">stddev=stddev</span><span class="s0">,</span>
                            <span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">seed=seed)</span>


<span class="s0">def </span><span class="s1">random_uniform(shape</span><span class="s0">, </span><span class="s1">minval=</span><span class="s5">0.0</span><span class="s0">, </span><span class="s1">maxval=</span><span class="s5">1.0</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns a tensor with uniform distribution of values. 
 
    # Arguments 
        shape: A tuple of integers, the shape of tensor to create. 
        minval: A float, lower boundary of the uniform distribution 
            to draw samples. 
        maxval: A float, upper boundary of the uniform distribution 
            to draw samples. 
        dtype: String, dtype of returned tensor. 
        seed: Integer, random seed. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        seed = np.random.randint(<span class="s5">10e6</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">tf.random_uniform(shape</span><span class="s0">, </span><span class="s1">minval=minval</span><span class="s0">, </span><span class="s1">maxval=maxval</span><span class="s0">,</span>
                             <span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">seed=seed)</span>


<span class="s0">def </span><span class="s1">random_binomial(shape</span><span class="s0">, </span><span class="s1">p=</span><span class="s5">0.0</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns a tensor with random binomial distribution of values. 
 
    # Arguments 
        shape: A tuple of integers, the shape of tensor to create. 
        p: A float, `0. &lt;= p &lt;= 1`, probability of binomial distribution. 
        dtype: String, dtype of returned tensor. 
        seed: Integer, random seed. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        seed = np.random.randint(<span class="s5">10e6</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">tf.where(tf.random_uniform(shape</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">seed=seed) &lt;= p</span><span class="s0">,</span>
                    <span class="s1">tf.ones(shape</span><span class="s0">, </span><span class="s1">dtype=dtype)</span><span class="s0">,</span>
                    <span class="s1">tf.zeros(shape</span><span class="s0">, </span><span class="s1">dtype=dtype))</span>


<span class="s0">def </span><span class="s1">truncated_normal(shape</span><span class="s0">, </span><span class="s1">mean=</span><span class="s5">0.0</span><span class="s0">, </span><span class="s1">stddev=</span><span class="s5">1.0</span><span class="s0">, </span><span class="s1">dtype=</span><span class="s0">None, </span><span class="s1">seed=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Returns a tensor with truncated random normal distribution of values. 
 
    The generated values follow a normal distribution 
    with specified mean and standard deviation, 
    except that values whose magnitude is more than 
    two standard deviations from the mean are dropped and re-picked. 
 
    # Arguments 
        shape: A tuple of integers, the shape of tensor to create. 
        mean: Mean of the values. 
        stddev: Standard deviation of the values. 
        dtype: String, dtype of returned tensor. 
        seed: Integer, random seed. 
 
    # Returns 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s1">:</span>
        dtype = floatx()
    <span class="s0">if </span><span class="s1">seed </span><span class="s0">is None</span><span class="s1">:</span>
        seed = np.random.randint(<span class="s5">10e6</span><span class="s1">)</span>
    <span class="s0">return </span><span class="s1">tf.truncated_normal(shape</span><span class="s0">, </span><span class="s1">mean</span><span class="s0">, </span><span class="s1">stddev</span><span class="s0">, </span><span class="s1">dtype=dtype</span><span class="s0">, </span><span class="s1">seed=seed)</span>


<span class="s2"># CTC</span>
# tensorflow has a native implemenation, but it uses sparse tensors
# and therefore requires a wrapper for Keras. The functions below convert
# dense to sparse tensors and also wraps up the beam search code that is
# in tensorflow's CTC implementation


<span class="s0">def </span><span class="s1">ctc_label_dense_to_sparse(labels</span><span class="s0">, </span><span class="s1">label_lengths):</span>
    <span class="s4">&quot;&quot;&quot;Converts CTC labels from dense to sparse. 
 
    # Arguments 
        labels: dense CTC labels. 
        label_lengths: length of the labels. 
 
    # Returns 
        A sparse tensor representation of the lablels. 
    &quot;&quot;&quot;</span>
    <span class="s1">label_shape = tf.shape(labels)</span>
    num_batches_tns = tf.stack([label_shape[<span class="s5">0</span><span class="s1">]])</span>
    max_num_labels_tns = tf.stack([label_shape[<span class="s5">1</span><span class="s1">]])</span>

    <span class="s0">def </span><span class="s1">range_less_than(_</span><span class="s0">, </span><span class="s1">current_input):</span>
        <span class="s0">return </span><span class="s1">tf.expand_dims(tf.range(label_shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s0">, </span><span class="s5">0</span><span class="s1">) &lt; tf.fill(</span>
            max_num_labels_tns<span class="s0">, </span><span class="s1">current_input)</span>

    init = tf.cast(tf.fill([<span class="s5">1</span><span class="s0">, </span><span class="s1">label_shape[</span><span class="s5">1</span><span class="s1">]]</span><span class="s0">, </span><span class="s5">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">tf.bool)</span>
    dense_mask = functional_ops.scan(range_less_than<span class="s0">, </span><span class="s1">label_lengths</span><span class="s0">,</span>
                                     <span class="s1">initializer=init</span><span class="s0">, </span><span class="s1">parallel_iterations=</span><span class="s5">1</span><span class="s1">)</span>
    dense_mask = dense_mask[:<span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s1">:]</span>

    label_array = tf.reshape(tf.tile(tf.range(<span class="s5">0</span><span class="s0">, </span><span class="s1">label_shape[</span><span class="s5">1</span><span class="s1">])</span><span class="s0">, </span><span class="s1">num_batches_tns)</span><span class="s0">,</span>
                             <span class="s1">label_shape)</span>
    label_ind = tf.boolean_mask(label_array<span class="s0">, </span><span class="s1">dense_mask)</span>

    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(<span class="s5">0</span><span class="s0">, </span><span class="s1">label_shape[</span><span class="s5">0</span><span class="s1">])</span><span class="s0">,</span>
                                                  <span class="s1">max_num_labels_tns)</span><span class="s0">, </span><span class="s1">reverse(label_shape</span><span class="s0">, </span><span class="s5">0</span><span class="s1">)))</span>
    batch_ind = tf.boolean_mask(batch_array<span class="s0">, </span><span class="s1">dense_mask)</span>
    indices = tf.transpose(tf.reshape(concatenate([batch_ind<span class="s0">, </span><span class="s1">label_ind]</span><span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span><span class="s0">, </span><span class="s1">[</span><span class="s5">2</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s1">]))</span>

    vals_sparse = tf.gather_nd(labels<span class="s0">, </span><span class="s1">indices)</span>

    <span class="s0">return </span><span class="s1">tf.SparseTensor(tf.to_int64(indices)</span><span class="s0">, </span><span class="s1">vals_sparse</span><span class="s0">, </span><span class="s1">tf.to_int64(label_shape))</span>


<span class="s0">def </span><span class="s1">ctc_batch_cost(y_true</span><span class="s0">, </span><span class="s1">y_pred</span><span class="s0">, </span><span class="s1">input_length</span><span class="s0">, </span><span class="s1">label_length):</span>
    <span class="s4">&quot;&quot;&quot;Runs CTC loss algorithm on each batch element. 
 
    # Arguments 
        y_true: tensor `(samples, max_string_length)` 
            containing the truth labels. 
        y_pred: tensor `(samples, time_steps, num_categories)` 
            containing the prediction, or output of the softmax. 
        input_length: tensor `(samples, 1)` containing the sequence length for 
            each batch item in `y_pred`. 
        label_length: tensor `(samples, 1)` containing the sequence length for 
            each batch item in `y_true`. 
 
    # Returns 
        Tensor with shape (samples,1) containing the 
            CTC loss of each element. 
    &quot;&quot;&quot;</span>
    <span class="s1">label_length = tf.to_int32(tf.squeeze(label_length))</span>
    input_length = tf.to_int32(tf.squeeze(input_length))
    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true<span class="s0">, </span><span class="s1">label_length))</span>

    y_pred = tf.log(tf.transpose(y_pred<span class="s0">, </span><span class="s1">perm=[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]) + </span><span class="s5">1e-8</span><span class="s1">)</span>

    <span class="s0">return </span><span class="s1">tf.expand_dims(ctc.ctc_loss(inputs=y_pred</span><span class="s0">,</span>
                                       <span class="s1">labels=sparse_labels</span><span class="s0">,</span>
                                       <span class="s1">sequence_length=input_length)</span><span class="s0">, </span><span class="s5">1</span><span class="s1">)</span>


<span class="s0">def </span><span class="s1">ctc_decode(y_pred</span><span class="s0">, </span><span class="s1">input_length</span><span class="s0">, </span><span class="s1">greedy=</span><span class="s0">True, </span><span class="s1">beam_width=</span><span class="s5">100</span><span class="s0">,</span>
               <span class="s1">top_paths=</span><span class="s5">1</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Decodes the output of a softmax. 
 
    Can use either greedy search (also known as best path) 
    or a constrained dictionary search. 
 
    # Arguments 
        y_pred: tensor `(samples, time_steps, num_categories)` 
            containing the prediction, or output of the softmax. 
        input_length: tensor `(samples, )` containing the sequence length for 
            each batch item in `y_pred`. 
        greedy: perform much faster best-path search if `true`. 
            This does not use a dictionary. 
        beam_width: if `greedy` is `false`: a beam search decoder will be used 
            with a beam of this width. 
        top_paths: if `greedy` is `false`, 
            how many of the most probable paths will be returned. 
 
    # Returns 
        Tuple: 
            List: if `greedy` is `true`, returns a list of one element that 
                contains the decoded sequence. 
                If `false`, returns the `top_paths` most probable 
                decoded sequences. 
                Important: blank labels are returned as `-1`. 
            Tensor `(top_paths, )` that contains 
                the log probability of each decoded sequence. 
    &quot;&quot;&quot;</span>
    <span class="s1">y_pred = tf.log(tf.transpose(y_pred</span><span class="s0">, </span><span class="s1">perm=[</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s1">]) + </span><span class="s5">1e-8</span><span class="s1">)</span>
    input_length = tf.to_int32(input_length)

    <span class="s0">if </span><span class="s1">greedy:</span>
        (decoded<span class="s0">, </span><span class="s1">log_prob) = ctc.ctc_greedy_decoder(</span>
            inputs=y_pred<span class="s0">,</span>
            <span class="s1">sequence_length=input_length)</span>
    <span class="s0">else</span><span class="s1">:</span>
        (decoded<span class="s0">, </span><span class="s1">log_prob) = ctc.ctc_beam_search_decoder(</span>
            inputs=y_pred<span class="s0">,</span>
            <span class="s1">sequence_length=input_length</span><span class="s0">, </span><span class="s1">beam_width=beam_width</span><span class="s0">,</span>
            <span class="s1">top_paths=top_paths)</span>

    decoded_dense = [tf.sparse_to_dense(st.indices<span class="s0">, </span><span class="s1">st.dense_shape</span><span class="s0">, </span><span class="s1">st.values</span><span class="s0">, </span><span class="s1">default_value=-</span><span class="s5">1</span><span class="s1">)</span>
                     <span class="s0">for </span><span class="s1">st </span><span class="s0">in </span><span class="s1">decoded]</span>
    <span class="s0">return </span><span class="s1">(decoded_dense</span><span class="s0">, </span><span class="s1">log_prob)</span>


<span class="s2"># HIGH ORDER FUNCTIONS</span>

<span class="s0">def </span><span class="s1">map_fn(fn</span><span class="s0">, </span><span class="s1">elems</span><span class="s0">, </span><span class="s1">name=</span><span class="s0">None, </span><span class="s1">dtype=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Map the function fn over the elements elems and return the outputs. 
 
    # Arguments 
        fn: Callable that will be called upon each element in elems 
        elems: tensor 
        name: A string name for the map node in the graph 
        dtype: Output data type. 
 
    # Returns 
        Tensor with dtype `dtype`. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.map_fn(fn</span><span class="s0">, </span><span class="s1">elems</span><span class="s0">, </span><span class="s1">name=name</span><span class="s0">, </span><span class="s1">dtype=dtype)</span>


<span class="s0">def </span><span class="s1">foldl(fn</span><span class="s0">, </span><span class="s1">elems</span><span class="s0">, </span><span class="s1">initializer=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Reduce elems using fn to combine them from left to right. 
 
    # Arguments 
        fn: Callable that will be called upon each element in elems and an 
            accumulator, for instance `lambda acc, x: acc + x` 
        elems: tensor 
        initializer: The first value used (`elems[0]` in case of None) 
        name: A string name for the foldl node in the graph 
 
    # Returns 
        Tensor with same type and shape as `initializer`. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.foldl(fn</span><span class="s0">, </span><span class="s1">elems</span><span class="s0">, </span><span class="s1">initializer=initializer</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">foldr(fn</span><span class="s0">, </span><span class="s1">elems</span><span class="s0">, </span><span class="s1">initializer=</span><span class="s0">None, </span><span class="s1">name=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Reduce elems using fn to combine them from right to left. 
 
    # Arguments 
        fn: Callable that will be called upon each element in elems and an 
            accumulator, for instance `lambda acc, x: acc + x` 
        elems: tensor 
        initializer: The first value used (`elems[-1]` in case of None) 
        name: A string name for the foldr node in the graph 
 
    # Returns 
        Same type and shape as initializer 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">tf.foldr(fn</span><span class="s0">, </span><span class="s1">elems</span><span class="s0">, </span><span class="s1">initializer=initializer</span><span class="s0">, </span><span class="s1">name=name)</span>


<span class="s0">def </span><span class="s1">local_conv1d(inputs</span><span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">kernel_size</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Apply 1D conv with un-shared weights. 
 
    # Arguments 
        inputs: 3D tensor with shape: (batch_size, steps, input_dim) 
        kernel: the unshared weight for convolution, 
                with shape (output_length, feature_dim, filters) 
        kernel_size: a tuple of a single integer, 
                     specifying the length of the 1D convolution window 
        strides: a tuple of a single integer, 
                 specifying the stride length of the convolution 
        data_format: the data format, channels_first or channels_last 
 
    # Returns 
        the tensor after 1d conv with un-shared weights, with shape (batch_size, output_lenght, filters) 
 
    # Raises 
        ValueError: if `data_format` is neither `channels_last` or `channels_first`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    stride = strides[<span class="s5">0</span><span class="s1">]</span>
    kernel_shape = int_shape(kernel)
    output_length<span class="s0">, </span><span class="s1">feature_dim</span><span class="s0">, </span><span class="s1">filters = kernel_shape</span>

    xs = []
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(output_length):</span>
        slice_length = slice(i * stride<span class="s0">,</span>
                             <span class="s1">i * stride + kernel_size[</span><span class="s5">0</span><span class="s1">])</span>
        xs.append(reshape(inputs[:<span class="s0">, </span><span class="s1">slice_length</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">,</span>
                          <span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">feature_dim)))</span>
    x_aggregate = concatenate(xs<span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    <span class="s2"># Shape: `(output_length, batch_size, filters)`.</span>
    <span class="s1">output = batch_dot(x_aggregate</span><span class="s0">, </span><span class="s1">kernel)</span>
    <span class="s0">return </span><span class="s1">permute_dimensions(output</span><span class="s0">, </span><span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">2</span><span class="s1">))</span>


<span class="s0">def </span><span class="s1">local_conv2d(inputs</span><span class="s0">, </span><span class="s1">kernel</span><span class="s0">, </span><span class="s1">kernel_size</span><span class="s0">, </span><span class="s1">strides</span><span class="s0">, </span><span class="s1">output_shape</span><span class="s0">, </span><span class="s1">data_format=</span><span class="s0">None</span><span class="s1">):</span>
    <span class="s4">&quot;&quot;&quot;Apply 2D conv with un-shared weights. 
 
    # Arguments 
        inputs: 4D tensor with shape: 
                (batch_size, filters, new_rows, new_cols) 
                if data_format='channels_first' 
                or 4D tensor with shape: 
                (batch_size, new_rows, new_cols, filters) 
                if data_format='channels_last'. 
        kernel: the unshared weight for convolution, 
                with shape (output_items, feature_dim, filters) 
        kernel_size: a tuple of 2 integers, specifying the 
                     width and height of the 2D convolution window. 
        strides: a tuple of 2 integers, specifying the strides 
                 of the convolution along the width and height. 
        output_shape: a tuple with (output_row, output_col) 
        data_format: the data format, channels_first or channels_last 
 
    # Returns 
        A 4d tensor with shape: 
        (batch_size, filters, new_rows, new_cols) 
        if data_format='channels_first' 
        or 4D tensor with shape: 
        (batch_size, new_rows, new_cols, filters) 
        if data_format='channels_last'. 
 
    # Raises 
        ValueError: if `data_format` is neither 
                    `channels_last` or `channels_first`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">is None</span><span class="s1">:</span>
        data_format = image_data_format()
    <span class="s0">if </span><span class="s1">data_format </span><span class="s0">not in </span><span class="s1">{</span><span class="s3">'channels_first'</span><span class="s0">, </span><span class="s3">'channels_last'</span><span class="s1">}:</span>
        <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">'Unknown data_format ' </span><span class="s1">+ str(data_format))</span>

    stride_row<span class="s0">, </span><span class="s1">stride_col = strides</span>
    output_row<span class="s0">, </span><span class="s1">output_col = output_shape</span>
    kernel_shape = int_shape(kernel)
    _<span class="s0">, </span><span class="s1">feature_dim</span><span class="s0">, </span><span class="s1">filters = kernel_shape</span>

    xs = []
    <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range(output_row):</span>
        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(output_col):</span>
            slice_row = slice(i * stride_row<span class="s0">,</span>
                              <span class="s1">i * stride_row + kernel_size[</span><span class="s5">0</span><span class="s1">])</span>
            slice_col = slice(j * stride_col<span class="s0">,</span>
                              <span class="s1">j * stride_col + kernel_size[</span><span class="s5">1</span><span class="s1">])</span>
            <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
                xs.append(reshape(inputs[:<span class="s0">, </span><span class="s1">:</span><span class="s0">, </span><span class="s1">slice_row</span><span class="s0">, </span><span class="s1">slice_col]</span><span class="s0">,</span>
                                  <span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">feature_dim)))</span>
            <span class="s0">else</span><span class="s1">:</span>
                xs.append(reshape(inputs[:<span class="s0">, </span><span class="s1">slice_row</span><span class="s0">, </span><span class="s1">slice_col</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">,</span>
                                  <span class="s1">(</span><span class="s5">1</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">feature_dim)))</span>

    x_aggregate = concatenate(xs<span class="s0">, </span><span class="s1">axis=</span><span class="s5">0</span><span class="s1">)</span>
    output = batch_dot(x_aggregate<span class="s0">, </span><span class="s1">kernel)</span>
    output = reshape(output<span class="s0">,</span>
                     <span class="s1">(output_row</span><span class="s0">, </span><span class="s1">output_col</span><span class="s0">, </span><span class="s1">-</span><span class="s5">1</span><span class="s0">, </span><span class="s1">filters))</span>

    <span class="s0">if </span><span class="s1">data_format == </span><span class="s3">'channels_first'</span><span class="s1">:</span>
        output = permute_dimensions(output<span class="s0">, </span><span class="s1">(</span><span class="s5">2</span><span class="s0">, </span><span class="s5">3</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s1">))</span>
    <span class="s0">else</span><span class="s1">:</span>
        output = permute_dimensions(output<span class="s0">, </span><span class="s1">(</span><span class="s5">2</span><span class="s0">, </span><span class="s5">0</span><span class="s0">, </span><span class="s5">1</span><span class="s0">, </span><span class="s5">3</span><span class="s1">))</span>
    <span class="s0">return </span><span class="s1">output</span>
</pre>
</body>
</html>